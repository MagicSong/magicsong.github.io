<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Hello World</title>
      <link href="/2018/06/20/hello-world/"/>
      <url>/2018/06/20/hello-world/</url>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>如何恢复失联的standby数据库</title>
      <link href="/2018/04/27/%E5%A6%82%E4%BD%95%E6%81%A2%E5%A4%8D%E5%A4%B1%E8%81%94%E7%9A%84standby%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
      <url>/2018/04/27/%E5%A6%82%E4%BD%95%E6%81%A2%E5%A4%8D%E5%A4%B1%E8%81%94%E7%9A%84standby%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
      <content type="html"><![CDATA[<blockquote><p>DMS的DG备库失联很久了，导致将standby数据库重新打卡时，发现apply出现了gap，即有一段归档日志已经没有了，但是却还没有应用。这篇文章讲述一下如何恢复这个备库。</p></blockquote><p>DMS的备库基本上用不到，平时只是作为一个数据库备份，所以可以随意操作。由于出现了Gap，所以需要重做standby备库，但是原本我学习的建立备库的方式是通过在主库RMAN上复制到备库。此次操作的原则就是要不影响主库，所以尝试使用主库的RMAN文件进行恢复。<br><strong>！！！！注意点</strong>：下面所有的操作都假设主库和备库的文件夹结构是<strong>一模一样</strong>的，如果不是，请参考此文<a href="https://blogs.oracle.com/database4cn/rmandataguardstandby" target="_blank" rel="noopener">https://blogs.oracle.com/database4cn/rmandataguardstandby</a>中间一段的说明进行配置！<br><a id="more"></a></p><h1 id="1-恢复standby控制文件"><a href="#1-恢复standby控制文件" class="headerlink" title="1. 恢复standby控制文件"></a>1. 恢复standby控制文件</h1><p>由于standby控制文件”失修”已久，所以需要从主库那边复制一个控制文件过来（控制文件会记录文件夹结构，如果备库和主库不一样，在恢复前一定要修改数据文件位置）。在<code>主库</code>的RMAN中执行下面的语句（这也是主库唯一需要执行的语句，对主库影响忽略不计）：<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">RMAN&gt; BACKUP CURRENT CONTROLFILE FOR STANDBY FORMAT '/tmp/ForStandbyCTRL.bck';</span><br></pre></td></tr></table></figure></p><p>将上面的控制文件传输到备库。上述语句中的<strong>FOR STANDBY</strong>不能漏掉。然后将所有的备份包括归档日志也传输到备库（本次不需要，因为所有的备份在备库上都有一个拷贝）。在备库中执行下面的语句添加备份文件：<br><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">CATALOG <span class="keyword">START</span> <span class="keyword">WITH</span> <span class="string">'XXXXXXXX'</span>;</span><br></pre></td></tr></table></figure></p><p>上面的XXX就是备份的文件夹。弄好之后在RMAN中执行下面的语句导入控制文件：<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">RMAN&gt; SHUTDOWN IMMEDIATE ;</span><br><span class="line">RMAN&gt; STARTUP NOMOUNT;</span><br><span class="line">RMAN&gt; RESTORE STANDBY CONTROLFILE FROM '/tmp/ForStandbyCTRL.bck';</span><br><span class="line">RMAN&gt; SHUTDOWN;</span><br><span class="line">RMAN&gt; STARTUP MOUNT;</span><br></pre></td></tr></table></figure></p><p>导入控制文件需要nomount，恢复数据库需要mount，这里提醒一下。另外我这里踩了一个坑，恢复控制文件的时候没有输入STANDBY关键字，导致最后备库变成了一个独立的库，这里提醒一下，一定是<strong>RESTORE STANDBY CONTROLFILE</strong>。</p><h1 id="2-恢复数据库"><a href="#2-恢复数据库" class="headerlink" title="2. 恢复数据库"></a>2. 恢复数据库</h1><p>下面开始恢复数据库，恢复数据库之前先查询一下归档日志到哪个SCN或者Sequence了（本次肯定是不完全恢复了，因为控制文件和备份都不是一起的，但是对最终结果没有影响，因为所有丢失的数据都会通过apply的方式找回）。用下面的语句查询：<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">RMAN&gt; LIST BACKUP OF ARCHIVELOG ALL;</span><br></pre></td></tr></table></figure></p><p>我的输出如下(只截取了最后一部分)：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">BS Key  Size       Device Type Elapsed Time Completion Time</span><br><span class="line">------- ---------- ----------- ------------ ---------------</span><br><span class="line">8134    81.08M     DISK        00:00:01     26-APR-18      </span><br><span class="line">        BP Key: 41094   Status: AVAILABLE  Compressed: NO  Tag: WEEKLY INC1 BACKUP</span><br><span class="line">        Piece Name: /u1/db/oracle/rmanbak/20180426_inc1_02t18vah_1_1.bkp</span><br><span class="line"></span><br><span class="line">  List of Archived Logs in backup set 8134</span><br><span class="line">  Thrd Seq     Low SCN    Low Time  Next SCN   Next Time</span><br><span class="line">  ---- ------- ---------- --------- ---------- ---------</span><br><span class="line">  1    25746   1250471243 26-APR-18 1250475702 26-APR-18</span><br><span class="line">  1    25747   1250475702 26-APR-18 1250479196 26-APR-18</span><br><span class="line">  1    25748   1250479196 26-APR-18 1250481461 26-APR-18</span><br></pre></td></tr></table></figure></p><p>可以看到最后的sequence是25748，然后执行不完全恢复，命令如下：<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">run  &#123;</span><br><span class="line">    <span class="keyword">set</span> <span class="keyword">until</span> <span class="keyword">sequence</span> <span class="number">25749</span>;</span><br><span class="line">    <span class="keyword">restore</span> <span class="keyword">database</span>;</span><br><span class="line">    recover database;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p>经过一段时间等待之后大功告成，然后进入SQLPLUS中将备库开启。开启的命令如下：<br><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">SHU IMMEDIATE;</span><br><span class="line">STARTUP MOUNT;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">DATABASE</span> <span class="keyword">OPEN</span> <span class="keyword">READ</span> <span class="keyword">ONLY</span>;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">DATABASE</span> <span class="keyword">RECOVER</span> <span class="keyword">MANAGED</span> <span class="keyword">STANDBY</span> <span class="keyword">DATABASE</span> <span class="keyword">DISCONNECT</span> <span class="keyword">FROM</span> <span class="keyword">SESSION</span>;</span><br></pre></td></tr></table></figure></p><p>然后查看一下备库是否起来：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SQL&gt; select open_mode from v$database;</span><br><span class="line"></span><br><span class="line">OPEN_MODE</span><br><span class="line">--------------------</span><br><span class="line">READ ONLY WITH APPLY</span><br><span class="line"></span><br><span class="line">SQL&gt; SELECT sequence#, first_time, next_time, applied FROM v$archived_log ORDER BY sequence#;</span><br><span class="line"></span><br><span class="line"> SEQUENCE# FIRST_TIM NEXT_TIME APPLIED</span><br><span class="line">---------- --------- --------- ---------</span><br><span class="line">     25746 26-APR-18 26-APR-18 YES</span><br><span class="line">     25747 26-APR-18 26-APR-18 YES</span><br><span class="line">     25748 26-APR-18 26-APR-18 YES</span><br><span class="line">     25749 26-APR-18 26-APR-18 YES</span><br><span class="line">     25750 26-APR-18 26-APR-18 YES</span><br><span class="line">     25751 26-APR-18 26-APR-18 YES</span><br><span class="line">     25752 26-APR-18 26-APR-18 YES</span><br><span class="line">     25753 26-APR-18 26-APR-18 YES</span><br><span class="line">     25754 26-APR-18 26-APR-18 YES</span><br><span class="line">     25755 26-APR-18 26-APR-18 YES</span><br><span class="line">     25756 26-APR-18 26-APR-18 YES</span><br></pre></td></tr></table></figure></p><p>过一段时间看到全部APPLY的时候就表示已经成功恢复了。</p><h1 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h1><p>本文介绍了如何利用备份重建STANDBY数据库，当然，如果能够在主库进行操作，那么我们只需要将丢失的那部分数据通过增量备份的方式传输给备库就可以了，这样的话备库就不需要像上面一样进行整库还原了（DMS整库还原很慢）。此次由于不想影响主库所以没有尝试这个操作，下次可以试试。</p>]]></content>
      
      
        <tags>
            
            <tag> dataguard </tag>
            
            <tag> restore </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>IT_Docker项目调研报告v1</title>
      <link href="/2018/04/24/IT_Docker%E9%A1%B9%E7%9B%AE%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8Av1/"/>
      <url>/2018/04/24/IT_Docker%E9%A1%B9%E7%9B%AE%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8Av1/</url>
      <content type="html"><![CDATA[<h1 id="一、项目介绍"><a href="#一、项目介绍" class="headerlink" title="一、项目介绍"></a>一、项目介绍</h1><p>目前IT课的大部分项目都有一个对应的开发测试环境，有一些是单独的一个服务器，其他一些共享一个服务器。随着项目越来越多，这些开发测试环境越来越多，DBA管理和维护这些系统的负担越来越重；一些系统共享一个服务器，但无法共享同一个端口和文件目录，使得开发和测试环境和正式的生产环境存在一些差异（环境一致性问题），无法快速实现持续集成；Docker容器化技术能够IT课开发人员提供一个简单、安全和轻量化的虚拟系统，保证各个应用隔离性同时，缩短了开发人员的开发周期。而DBA只需要负责管理和维护容器和镜像，从而提高IT部门生产力。<br><a id="more"></a></p><hr><h1 id="二、项目完成进度"><a href="#二、项目完成进度" class="headerlink" title="二、项目完成进度"></a>二、项目完成进度</h1><h2 id="项目预定目标"><a href="#项目预定目标" class="headerlink" title="项目预定目标"></a>项目预定目标</h2><ul><li style="list-style: none"><input type="checkbox" checked> 1. docker的安装部署</li><li style="list-style: none"><input type="checkbox" checked> 2. 单机docker下的自动化集成测试</li><li style="list-style: none"><input type="checkbox" checked> 3. 迁移smbcommunity至docker </li></ul><h2 id="额外功能调研"><a href="#额外功能调研" class="headerlink" title="额外功能调研"></a>额外功能调研</h2><ul><li style="list-style: none"><input type="checkbox" checked> 1. docker swarm功能调研</li><li style="list-style: none"><input type="checkbox" checked> 2. kubernetes 功能调研</li><li style="list-style: none"><input type="checkbox" checked> 3. 分布式集群存储功能</li><li style="list-style: none"><input type="checkbox" checked> 4. 集群环境下的自动化集成</li></ul><h2 id="进度分析"><a href="#进度分析" class="headerlink" title="进度分析"></a>进度分析</h2><p>本项目从2018年2月22日开始申请，预计到4月30日结束，项目中大部分目标都提前完成了，提前完成主要原因并不是效率高，而是对于其目标估计不够。具体的说明如下表。</p><table><thead><tr><th>主要工作节点</th><th>开始时间</th><th>完成时间</th><th>预计耗时</th><th>实际耗时</th><th>延期原因简要说明</th></tr></thead><tbody><tr><td>docker学习和部署</td><td>2-22</td><td>3-7</td><td>10天</td><td>10天</td><td>docker的安装是非常简单的，但是申请机器，然后到权限开通期间耗费了大部分时间。考虑到本项目是一个测试项目，无法分配过多人力，实际耗时还算正常</td></tr><tr><td>自动化集成部署</td><td>3-7</td><td>4-10</td><td>30天</td><td>18天</td><td>由于事先并不熟悉一个Tomcat项目开发到部署的完整流程，多花了一些时间。实际用了18天主要是因为前期在等待机器权限的时候自学了部分内容。</td></tr><tr><td>docker集群学习</td><td>4-1</td><td>至今</td><td>20天</td><td>——</td><td>docker的主流应用还是在分布式服务上，出于个人技能提升的目的学习了分布式docker集群的相关知识。</td></tr></tbody></table><hr><h1 id="三、-项目成果展示"><a href="#三、-项目成果展示" class="headerlink" title="三、 项目成果展示"></a>三、 项目成果展示</h1><h2 id="docker-部署"><a href="#docker-部署" class="headerlink" title="docker 部署"></a>docker 部署</h2><p>部署环境：CentOS 7.4 x64<br>部署文档：<a href="https://doc.itdocker.rd.tp-link.net/2018/03/14/Docker%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97/" target="_blank" rel="noopener">https://doc.itdocker.rd.tp-link.net/2018/03/14/Docker%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97/</a><br>部署截图：截图上显示已经部署了43个容器，使用了35个镜像  </p><p><img src="./2018-04-24_105556.jpg" alt="docker 管理UI"></p><h2 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h2><p>部署环境：三台CentOS 7.4 x64服务器<br>集群部署方案：<strong>kubernetes</strong><br>部署文档：<a href="https://doc.itdocker.rd.tp-link.net/2018/04/08/%E7%A7%91%E5%AD%A6%E5%AE%89%E8%A3%85%E6%9C%80%E6%96%B0%E7%89%88kubernetes/" target="_blank" rel="noopener">https://doc.itdocker.rd.tp-link.net/2018/04/08/%E7%A7%91%E5%AD%A6%E5%AE%89%E8%A3%85%E6%9C%80%E6%96%B0%E7%89%88kubernetes/</a><br>部署截图：<br><img src="./2018-04-24_110236.jpg" alt="集群管理 UI"></p><h2 id="自动化持续集成测试"><a href="#自动化持续集成测试" class="headerlink" title="自动化持续集成测试"></a>自动化持续集成测试</h2><p>部署文档：<a href="https://doc.itdocker.rd.tp-link.net/2018/03/15/%E5%9C%A8docker%E4%B8%AD%E9%85%8D%E7%BD%AEjenkins/" target="_blank" rel="noopener">https://doc.itdocker.rd.tp-link.net/2018/03/15/%E5%9C%A8docker%E4%B8%AD%E9%85%8D%E7%BD%AEjenkins/</a><br>应用地址：<a href="https://jenkins.itdocker.rd.tp-link.net" target="_blank" rel="noopener">https://jenkins.itdocker.rd.tp-link.net</a><br>jenkins安装截图：<br>jenkins首页 <img src="./2018-04-24_110644.jpg" alt="jenkins首页"><br>jenkins 自动化部署SMBCommunity <img src="./2018-04-24_111331.jpg" alt="">     </p><h2 id="glusterfs-分布式存储"><a href="#glusterfs-分布式存储" class="headerlink" title="glusterfs 分布式存储"></a>glusterfs 分布式存储</h2><p>说明：分布式集群中服务无法指定在特定的node节点工作，所以服务存储的地方应该和主机独立开来。通常会考虑网络存储，但介于ITDocker 是一个测试项目，不需要那么多优良的硬件资源，所以考虑利用自身硬盘组建一块分布式硬盘。glusterfs 是一个分布式存储解决方案，能够利用较多廉价服务器的硬盘空间，组成一块能够自备份自修复的硬盘，而且横向扩展方便，可以很方便地加入任意的节点。ITDocker在测试中使用了glusterfs，用于保证服务的一致性。这也是对分布式存储的一次调研。对DMS和OA系统以后的优化可能具有参考价值。</p><h2 id="其他测试应用"><a href="#其他测试应用" class="headerlink" title="其他测试应用"></a>其他测试应用</h2><h3 id="集群监控Grafana"><a href="#集群监控Grafana" class="headerlink" title="集群监控Grafana"></a>集群监控Grafana</h3><p>地址：<a href="https://monitor.itdocker.rd.tp-link.net" target="_blank" rel="noopener">https://monitor.itdocker.rd.tp-link.net</a><br>应用截图：<img src="./2018-04-24_112026.jpg" alt="Grafana截图"></p><h3 id="docker私有仓库"><a href="#docker私有仓库" class="headerlink" title="docker私有仓库"></a>docker私有仓库</h3><p>仓库地址：<a href="https://registry.itdocker.rd.tp-link.net" target="_blank" rel="noopener">https://registry.itdocker.rd.tp-link.net</a><br>仓库管理工具地址：<a href="https://registry-ui.itdocker.rd.tp-link.net" target="_blank" rel="noopener">https://registry-ui.itdocker.rd.tp-link.net</a><br>管理工具截图：<img src="./2018-04-24_112433.jpg" alt="私有仓库管理"></p><h3 id="高可用redis主从集群"><a href="#高可用redis主从集群" class="headerlink" title="高可用redis主从集群"></a>高可用redis主从集群</h3><p>地址：TCP://172.29.41.127:26379<br>集群架构：后端为一主两从+四个哨兵，前端是nginx负载四个哨兵的Load Balancer和高可用性。<br>说明：在k8s上能够非常方便的建立一个redis主从集群，一个命令就可以为一个应用配置一个redis集群，同时还能测试redis集群的高可用性以及横向扩展能力。</p><hr><h1 id="四、-经验总结"><a href="#四、-经验总结" class="headerlink" title="四、 经验总结"></a>四、 经验总结</h1><h2 id="主要困难"><a href="#主要困难" class="headerlink" title="主要困难"></a>主要困难</h2><ol><li>docker的主要原则就是尽量一个容器里做一件事情（低耦合），但是现有的一些项目各个模块之间耦合度太高。比如一个项目分前端和后端，为了方便在maven中装了一个前端打包插件，允许在maven中调用前端打包工具进行打包，这样一来，一次编译就能同时编译前端和后端。这种做法在单机环境是没有问题的，但是在docker下就比较为难。为此，实验时在一个Maven的镜像中安装了sencha，安装过程并不顺利。最后只能在ubuntu镜像上安装maven和sencha，总结起来就是docker为了能够完成一次性编译的任务，装了一个操作系统（占用较大空间，启动也慢）。但是如果能够做到前后端真正分离的话，一个sencha镜像和一个maven镜像就可以独立解决（而且完全可以是并行的）。</li><li>维护docker集群需要写很多配置文件，需要做很多测试，如果直接在服务器上写的话，vim效率低；在本地写然后同步到服务上再运行效率也低。为此个人研究出了一套基于WSL（windows subsystem for linux）+vscode的方案，文档地址在这里(<a href="https://doc.itdocker.rd.tp-link.net/2018/04/20/WSL-vscode%E5%8F%8C%E5%89%91%E5%90%88%E7%92%A7/)，在这个方案下能够提升系统管理员测试和部署效率。" target="_blank" rel="noopener">https://doc.itdocker.rd.tp-link.net/2018/04/20/WSL-vscode%E5%8F%8C%E5%89%91%E5%90%88%E7%92%A7/)，在这个方案下能够提升系统管理员测试和部署效率。</a></li><li>在部署kubernets的初期，成功搭建了一个集群之后，部署的应用只能在部署的机器上看到，集群中其他机器看不到，debug了两天才发现是节点之间UDP协议不同。当初申请文档的时候错将节点之间互相访问无限制的协议填了TCP，浪费了很多时间在这个错误排除上。以后要部署集群的时候一定要注意这个点，互访无限制要包括任何协议。网上的教程默认都不会考虑到这个限制，所以很少有人会出现这个问题，搜索了半天也搜索不到答案。</li><li>k8s对安全的要求比较高，因为上面任何一个部署的容器都可以轻而易举获得root权限（通过挂载<code>/</code>目录）。许多应用部署都强制要求https，以前那种不考虑安全传输的部署方式行不通。学习了一些安全相关的知识，包括SSL和Access Control等才能开始正确部署应用。</li></ol><h2 id="技术积累"><a href="#技术积累" class="headerlink" title="技术积累"></a>技术积累</h2><p>本次调研算是初步完成了，下面梳理一下个人理解下的Docker对于IT课业务影响。</p><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li>DBA能够统一管理应用程序。如果有人问IT课开发与测试环境上有哪些应用，这个问题基本上不会有答案。docker能够轻而易举给出这个问题的答案。其次，docker能够给每一个应用分配相应的资源，提醒开发者程序的资源是有限的，从而写出优秀的代码。DBA也能更方便地管理手上的计算资源和存储资源，学会合理地进行资源分配，对一些不活动的容器进行限制，提高服务器资源利用率。</li><li>硬件资源管理方便。如果docker性能真的遇到了瓶颈，需要申请服务器资源了，只需要将集群中的节点克隆一个就好了。</li><li>开发人员只要提交代码，通过审核，就会自动有一个新的容器生产，提高了开发效率。</li><li>分布集群下的开发讲究低耦合高内聚，因为谁也不清楚开发的应用部署在哪个节点。DMS系统就是由于很多东西写在了一起，导致后续流量上来了出现问题难以维护。所以在这种开发模式下，开发者会自己提高应用程序的内聚，让自己的应用程序打包成镜像就能在任意的节点运行。</li><li>分布式集群下存储和单机也不同，集群下的存储是所有节点共享的，并且和主机分离（很多时候并不会挂载在主机上，而是让容器去挂载），如果DMS和OA采用这种模式的话，那么DMS系统就能专心处理业务逻辑，文件的备份交给存储，文件就不需要在节点间同步同时也能保证容灾性。</li><li>在docker上开发的应用可以在任何能够运行docker的系统上运行，包括windows。</li><li>docker集群上应用能够统一监控，日志的统一管理，后续业务上来了，可以对日志进行信息挖掘，针对热门请求优化后端逻辑。</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li>分布式集群下对开发人员的代码能力有了更高的要求，高内聚低耦合的应用更适合在集群上跑，这会对现有业务代码产生影响。开发人员需要写更好甚至有可能更多的代码维护低耦合。这需要更多的精力和时间。</li><li>管理分布式集群需要DBA良好的架构能力，对一些应用统一规划，诸如Tomcat应用日志等。同时对资源的调度也需要更多的经验，不至于经常出现OutOfMemory的错误。</li><li>维护一个集群需要写很多脚本，DBA也需要有强大的Coding能力。</li></ol><p><strong>架构是为业务服务的，上述三点在人力不足的情况下很难满足，当前IT课的开发还算处于稳定状态，IT课业务是否需要部署集群还有待商榷。</strong></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>总的来说，Docker是一个让人感到惊艳的项目，它将应用隔离、部署以及资源管控都做到了一个相当可靠的程度。如果说docker是一把利剑，那么k8s就是一个善于用剑的武士，将docker的威力发挥到了极致。k8s将docker真正地带进了生产环境。如果公司的小型应用越来越多，可以考虑将k8s应用于正式环境。docker很适合我司的业务架构，在未来项目越来越多的时候，说不定可以考虑容器化公司的IT业务，将服务至于k8s云上。</p>]]></content>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> 项目总结 </tag>
            
            <tag> 容器化 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>WSL+vscode双剑合璧</title>
      <link href="/2018/04/20/WSL-vscode%E5%8F%8C%E5%89%91%E5%90%88%E7%92%A7/"/>
      <url>/2018/04/20/WSL-vscode%E5%8F%8C%E5%89%91%E5%90%88%E7%92%A7/</url>
      <content type="html"><![CDATA[<blockquote><p>今天下午捣鼓了一下WSL，通过一些配置将WSL改造了一下，结合vs code之后，windows系统看上去越来越像一个硬盘了。下面记录下我的操作。😈😈😈😈😈😈😈😈😈😈😈😈😈</p></blockquote><a id="more"></a><p>公司给的电脑很卡，同时开WSL+docker+vscode+chrome+nodejs就已经卡的不行了，所以必须改造一下。按照我之前的<a href="https://doc.itdocker.rd.tp-link.net/2018/04/19/%E5%9C%A8windows%E4%B8%8B%E5%81%9A%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4%E2%80%94%E4%B8%AA%E4%BA%BA%E5%BF%83%E5%BE%97/#more" target="_blank" rel="noopener">文章</a>配置好WSL之后，就需要在WSL装一些软件了。由于我的工作内容主要有三部分：</p><ol><li>文档</li><li>docker</li><li>k8s</li></ol><p>下面针对上面三个部分依依说明。</p><h1 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h1><p>我写文档用的markdown，word由于不支持语法高亮早早就被我抛弃了。markdown的框架用的是hexo，一个用nodejs写的静态博客框架。所以首先要在WSL中安装nodejs，记得不要用apt-get 安装，一大堆问题。直接从官网上拉编译好的二进制文件，<code>ln</code>到bin目录即可。需要<code>ln</code>的二进制文件主要是<code>node</code>和<code>npm</code>。在WSL中执行检查一下，输出如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/mnt/d/blog$ node --version</span><br><span class="line">v8.9.1</span><br><span class="line">/mnt/d/blog$ npm --version</span><br><span class="line">5.5.1</span><br></pre></td></tr></table></figure></p><p>Very Good。下面安装hexo，由于我司的服务器都不能上网，所以很多工作都是在离线环境下完成的。现在WSL是在windows下，终于有一个可以上网的linux啦！！ 🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣。执行下面的命令安装<strong>hexo</strong>:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo npm install -g hexo-cli</span><br></pre></td></tr></table></figure></p><p>安装完之后进入我的博客文件夹，输入npm install即可。这样一来文档就配置好了。但是这里有一个瑕疵，由于SWL的限制，hexo的<strong>SFTP</strong>用不了，其实也无所谓，因为后来发现根本用不到，我用了更好的<strong>rsync</strong>。输入<code>hexo list post</code>看一下吧。<img src="/2018/04/20/WSL-vscode双剑合璧/2018-04-20_173054.jpg" title="hexo安装完成"><br>安装完成之后还需要设置SSH免密登录，步骤是先在WSL<code>ssh-keygen -t rsa</code>生成公钥和密钥，然后将公钥传输（追加，不是覆盖）到服务器的<code>~/.ssh/authorized_keys</code>文件中，这样就完成了免密登录。</p><h1 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h1><p>我原本是在windows上安装docker的，主要目的有两个，一是去拉墙外的镜像，二是为了本地开发。去拉墙外的镜像现在已经通过一些途径解决，后面的就开始考虑用WSL代替。我的服务器是安装了docker的，现在就是要实现远程访问docker。首先要在服务器上开启这个功能，很简单，在docker.service中启动参数中添加<code>-H 0.0.0.0:2375</code>即可，添加后执行下面的命令重启docker：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure></p><p>重启之后就可以看到有docker进程在监听2375端口啦。接着从docker网站上下编译好的docker包，地址在→_→：<a href="https://download.docker.com/linux/static/stable/x86_64/。下载完成之后在WSL中解压缩，然后将其中的一个名称为docker的可执行文件拖到bin目录下即可，这是docker客户端，其实就是将命令转换restful" target="_blank" rel="noopener">https://download.docker.com/linux/static/stable/x86_64/。下载完成之后在WSL中解压缩，然后将其中的一个名称为docker的可执行文件拖到bin目录下即可，这是docker客户端，其实就是将命令转换restful</a> API然后将结果呈现在命令行的工具。其余的二进制目前还用不到，可以先放着。客户端好了还需要配置远程服务器，非常简单。下面一句话：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> DOCKER_HOST=X.X.X.X:2375</span><br></pre></td></tr></table></figure></p><p>XXXX是远程服务器的地址。把这个放入.profile文件即可。<code>source</code>一下.profile文件，看一下<code>docker info</code>的执行结果吧！ 🤭🤭🤭🤭🤭🤭🤭🤭🤭🤭🤭🤭🤭🤭<br><img src="/2018/04/20/WSL-vscode双剑合璧/2018-04-20_174901.jpg" title="docker info结果"></p><h1 id="kubernetes"><a href="#kubernetes" class="headerlink" title="kubernetes"></a>kubernetes</h1><p>需要访问k8s集群的话需要<code>kubectl</code>，基于我们已经安装好了docker，可以直接使用容器中的kubectl作为命令直接使用（相比原生命令慢不了多少的）。这里继续记录一下如何本地安装<strong>kubectl</strong>。和docker一样，直接从官网上下载编译好的二进制文件即可，然后+x就可以了。下面汇总一下命令：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kubectl</span><br><span class="line">chmod +x kubectl</span><br><span class="line">sudo mv ./kubectl /usr/<span class="built_in">local</span>/bin/kubectl</span><br></pre></td></tr></table></figure></p><p>客户端安装好之后，配置一下k8s服务器，很简单，只需要将k8s集群上的<code>admin.conf</code>拖到本地的<code>.kube/config</code>即可。输入<code>kubectl cluster-info</code>看一下吧。<img src="/2018/04/20/WSL-vscode双剑合璧/2018-04-20_175918.jpg" title="kubectl命令"></p><h1 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h1><p>一下午，终于把自己的工作环境弄的非常舒服了，现在基本上windows做系统运维都是用linux的操作了，装windows也基本上为一些个别需求了，windows上的docker也可以安静地躺在硬盘里，而不继续消耗我的内存了！</p>]]></content>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> WSL </tag>
            
            <tag> vs code </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何在k8s中部署一个web应用</title>
      <link href="/2018/04/19/%E5%A6%82%E4%BD%95%E5%9C%A8k8s%E4%B8%AD%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AAweb%E5%BA%94%E7%94%A8/"/>
      <url>/2018/04/19/%E5%A6%82%E4%BD%95%E5%9C%A8k8s%E4%B8%AD%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AAweb%E5%BA%94%E7%94%A8/</url>
      <content type="html"><![CDATA[<blockquote><p>这篇文章主要论述如何在k8s中部署一个应用，并且符合k8s规范。具体部署的内容因不同的应用而异，这里会罗列一些注意事项。<br><a id="more"></a></p></blockquote><h1 id="部署应用的流程"><a href="#部署应用的流程" class="headerlink" title="部署应用的流程"></a>部署应用的流程</h1><h2 id="什么是服务"><a href="#什么是服务" class="headerlink" title="什么是服务"></a>什么是服务</h2><p>kubernetes中的最小单位是POD，POD往往是一个和几个关联的容器（<code>container</code>）。原生docker往往直接暴露的就是容器，应用通过端口直接访问容器，但是这种方式不利于负载均衡和高可用，因为一旦这个容器挂掉了或者IP改了，都会造成应用不可用。k8s中是以service作为应用访问的起点，如下图：<img src="/2018/04/19/如何在k8s中部署一个web应用/services-iptables-overview.svg" title="k8s中的服务模型"></p><p>下面贴出了一个样例服务，来自于doc.itdocker.rd.tp-link.net上应用。<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span>  <span class="string">docker-docs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span>  <span class="string">docker-docs</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span>  <span class="string">http</span></span><br><span class="line"><span class="attr">    port:</span>  <span class="number">80</span></span><br><span class="line"><span class="attr">    targetPort:</span>  <span class="number">80</span></span><br></pre></td></tr></table></figure></p><p>上面的服务非常简单，因为大部分是默认属性了，这里论述一些关键属性。第一个是<code>selector</code>，<code>selector</code>表示选择这个service背后的实际后端的逻辑。这里的意思就是这个service选择后端具有app: docker-docs的pod，任何在这个命名空间里具有这个label的都会被选择。<code>ports</code>表示后端pod暴露的端口和自身端口的映射，其中<code>port</code>是服务自身暴露给外界的端口，<code>targetPort</code>就是后端pod的端口。这样一个服务就定义好了，它自身暴露了80端口，然后把经过自身80端口的流量转发给后端的POD处理（默认选择的方式是round-robin）。<strong>这里有个注意点</strong>，虽然service能够把流量转发到后端POD，但是当我们按照此文真正部署完一个应用之后，实际的流量是通过nginx控制的（名称叫ingress），service只是起一个后端发现的作用。当然还有一些其他的部署方式（不走本文的方法，用proxy，用nodePort），它们的流量是过service转发的。</p><h2 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h2><p>写好service之后就要写真正处理逻辑的后端，后端可以直接写<code>POD</code>，也可以用<code>Deployment</code>，本文推荐用<code>Deployment</code>，<code>Deployment</code>可以控制应用的可用数量，并且控制如何在集群节点上分布我们的应用。下面贴上一个我写的Deployment。<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">docker-docs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">docker-docs</span></span><br><span class="line"><span class="attr">        editor:</span> <span class="string">vscode</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      imagePullSecrets:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">pull-secret</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">docker-docs</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">rdsource.tp-link.net:8088/nginx:alpine</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">          - name:</span>  <span class="string">http</span></span><br><span class="line"><span class="attr">            containerPort:</span>  <span class="number">80</span></span><br><span class="line"><span class="attr">            protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">          volumeMounts:</span></span><br><span class="line"><span class="attr">            - name:</span>  <span class="string">document-storage</span></span><br><span class="line"><span class="attr">              mountPath:</span>  <span class="string">/usr/share/nginx/html</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">        - name:</span>  <span class="string">document-storage</span></span><br><span class="line"><span class="attr">          hostPath:</span></span><br><span class="line"><span class="attr">            path:</span>  <span class="string">/opt/docker_files/documents</span></span><br></pre></td></tr></table></figure></p><p>上面这个的Deployment简单部署了一个nginx应用（读取静态HTML并显示出来）。可以看到replica是2，说明会在集群中部署两个POD，作为一个简单的负载均衡和高可用。POD通过<code>containerPort</code>属性暴露了80端口，这个和上面的service中的<code>targetPort</code>是一致的，不一致会导致服务不可用。Deployment指定了需要的镜像和镜像名称。这里也有一个注意事项，就是私有仓库拉取。我司的私有仓库是一个代理，拉取镜像要有用户名和密码。在主机上操作一次时，只要用<code>docker login</code>一次，以后就不用再登录了，具体的登录信息会保存在<code>~/.docker/config.json</code>中。这个登录信息是会过期的，我在测试的时候一般是重启之后就会过期，需要重新登录。但是k8s不会使用上述登录信息，所以每次从私有仓库拉取镜像（如果需要用户名和密码）都必须指定secret，上面语句中的<code>imagePullSecrets</code>就是这个secret。secret的生成模板如下：</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span>  <span class="string">pull-secret</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="string">.dockerconfigjson:</span> <span class="string">XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">kubernetes.io/dockerconfigjson</span></span><br></pre></td></tr></table></figure><p>其中的<code>.dockerconfigjson</code>就是<code>cat ~/.docker/config.json</code>的输出结果。需要注意的是secret不能在不同的namespaces中共享，需要为每一个namespace建立一个secret（这个有点设计不合理了，我觉得）。</p><h2 id="访问服务"><a href="#访问服务" class="headerlink" title="访问服务"></a>访问服务</h2><p>至此，一个服务和它的后端都建立起来了。但是外界是无法访问这个服务的，而集群内部可以通过[serviceName].[namespace]的格式访问。大部分服务都是要提供给外界的，那么如果访问呢？有多种方式：</p><ol><li>在service中配置<ul><li>将service的type设置为<code>nodePort</code>，那么系统会随机为服务分配一个30000以上的端口号，然后通过任意节点+这个端口号进行访问。缺点很明显，端口号很大，不容易记住，而且是随机的。</li><li>将service的type设置为<code>loadBalancer</code>，让集群的云服务商提供一个外部ip作为VIP访问这个服务。这是在云服务中通用的，但是在私有集群中难以实现。</li><li>用<code>kubectl proxy</code>做代理，访问服务。缺点很明显，每次访问都要做代理，而且只有执行<code>kubectl proxy</code>的那个主机才能访问。但也有优点，非常安全。对于一些安全级别很高的应用，应当使用这个方式。</li></ul></li><li>用ingress<br><strong>ingress</strong>表示了一个应用的入口配置，它是通过<code>ingress controller</code>实现的。<strong>ingress controller</strong>的简介如下：<blockquote><p>Configuring a webserver or loadbalancer is harder than it should be. Most webserver configuration files are very similar. There are some applications that have weird little quirks that tend to throw a wrench in things, but for the most part you can apply the same logic to them and achieve a desired result.<br>The Ingress resource embodies this idea, and an Ingress controller is meant to handle all the quirks associated with a specific “class” of Ingress.<br>An Ingress Controller is a daemon, deployed as a Kubernetes Pod, that watches the apiserver’s /ingresses endpoint for updates to the Ingress resource. Its job is to satisfy requests for Ingresses.</p></blockquote></li></ol><p>ingress controller主要作用就是负载均衡和web应用配置，ingress controller本身也是一个服务，它将自身的特定端口暴露在外部世界中，外部通过域名访问时，ingress controller会通过域名规则将流量转发到指定的后端（Endpoints）。这里有一个小知识点，ingress controller是直接转发到后端POD上的，而不是转到服务上，大家知道服务是round-robin的方式选择后端的，而ingresscontroller可以实现更加灵活的选择方式，比如权重啊等等。看到这，大家肯定有疑问，这不就是<strong>nginx</strong>吗？哈哈，就是<strong>nginx</strong>实现的模式，k8s官方推荐的也是用nginx加工的，相比原生nginx，ingress controller最大的特点就是nginx的配置文件自动生成，我们不需要为所有的应用去写nginx语法，仅仅只需要一个ingress文件即可。所以要想使用ingress，就必须先安装ingress controller。安装过程见我其他文章（TODO）。<br>下面贴上我们的ingress:<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/proxy-read-timeout:</span> <span class="string">"600"</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/proxy-send-timeout:</span> <span class="string">"600"</span></span><br><span class="line">    <span class="string">kubernetes.io/tls-acme:</span> <span class="string">'true'</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">docker-docs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  tls:</span></span><br><span class="line"><span class="attr">  - hosts:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">doc.itdocker.rd.tp-link.net</span></span><br><span class="line"><span class="attr">    secretName:</span> <span class="string">itdocker-tls</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">doc.itdocker.rd.tp-link.net</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">docker-docs</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">        path:</span> <span class="string">/</span></span><br></pre></td></tr></table></figure></p><p>上述ingress是一个加持了https的配置，其中<code>host</code>和<code>hosts</code>属性指定了访问的域名，即用户通过<code>doc.itdocker.rd.tp-link.net</code>这个域名访问，ingress controller就能识别用户是想要访问后端的一个docker-docs的服务。这里虽然写了服务，但是实际流量是直接转发到后端POD上的哦，服务只是作为一个查询中介（这个小原理不知道也无所谓 😇😇😇😇😇😇）。<code>path</code>可以指定域名后的路径，我在使用后面路径的时候常常遇到404的问题，主要是因为用户通过带路径的域名访问时，一些应用的一些资源是用的绝对路径，比如我们ingress指定了<code>docker.net/a</code>转发到a应用，但是a应用中某个资源路径是从根目录开始的，比如/api/XXX，a应用这么请求肯定找不到资源，因为按照我们的ingress设置，路径应当为/a/api/XXX才能正确转发。有些应用支持baseURL属性，有了baseURL，它的所有请求都是带baseURL的，这种应用就可以通过路径的方式区别开来。而其他不支持baseURL的应用在这种模式下很容易出错。无奈之下，我申请了域名的子域名通配符，这样一来，我就可以按照我的设计为docker上的应用配置ingress和其域名。<br>上述ingress使用了SSL证书，所以访问这个应用时会自动跳转到ingress controller的443端口，SSL证书和密钥存放在上述<code>itdocker-tls</code>这个密钥中。可以看到https加密是在前端nginx做的，而不是后端，后端还是80端口。为了用户能够正确访问，需要将域名绑定到集群中的任意一个节点，我绑定到了我的master节点。当然也可以搞一个虚拟IP。虚拟IP的方式比较合理，我还没有尝试，下次可以试试。</p><p>至此，将上述所有文件apply一下，一个完整的应用就好了。下图整理了本文的思路，具体的流程就如下：<img src="/2018/04/19/如何在k8s中部署一个web应用/ingress.png" title="ingress controller原理简图"></p><h1 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h1><p>ingress controller是部署在L7,但是很多应用需要在L4上部署（mysql,redis,git clone），如何利用ingress暴露TCP服务我还要继续研究一下。</p>]]></content>
      
      
        <tags>
            
            <tag> 部署 </tag>
            
            <tag> k8s </tag>
            
            <tag> http </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>在windows下做系统运维—个人心得</title>
      <link href="/2018/04/19/%E5%9C%A8windows%E4%B8%8B%E5%81%9A%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4%E2%80%94%E4%B8%AA%E4%BA%BA%E5%BF%83%E5%BE%97/"/>
      <url>/2018/04/19/%E5%9C%A8windows%E4%B8%8B%E5%81%9A%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4%E2%80%94%E4%B8%AA%E4%BA%BA%E5%BF%83%E5%BE%97/</url>
      <content type="html"><![CDATA[<blockquote><p>随着学习的慢慢深入，越来越觉得在终端工作是一个很舒服的事情，虽然还是有很多工具离不开一些GUI，比如PLSQL，印象笔记，outlook等，但是这类的工具开始越来越少了。这里记录一下我的心得，记录一下我是如何打造一个属于我的工作环境的。</p></blockquote><a id="more"></a><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>虽然很想将自己的操作系统直接变成linux，但是由于公司很多东西都很依赖windows，所以暂时还是无法离开windows的。但是windows的爸爸微软对待开源世界也越来越开放了，对linux的态度也越来越好，对linux的支持真是越来越棒了。感受最深的就是vs code和WSL（Windows Subsystem for Linux），尤其是后者，随着WSL的越来越成熟，在windows下使用linux变得越来越简单，开发linux脚本和其他脚本都可以直接在WSL中运行，运行成功了之后可以部署在真正的服务器中。非常的方便。</p><p>这里想说明一下为什么不直接在服务器中做开发呢？这个还是个人原因，公司的服务器是无法联网的，服务器中的vim是无法安装插件的（大多数也不会去安装插件），对于一个习惯了在windows下编程的人来说（语法高亮，缩进支持，snippets），很多功能都用不了，心里微微有点不舒服。个人是还不习惯用vim，而且自从有了vs code之后，完全没有使用vim的动力。所以研究了一套在windows下做系统运维的方法，使用时间不长，但是目前还是很舒服的。</p><h1 id="安装必要工具"><a href="#安装必要工具" class="headerlink" title="安装必要工具"></a>安装必要工具</h1><h2 id="windows-10"><a href="#windows-10" class="headerlink" title="windows 10"></a>windows 10</h2><p>微软对开源世界的大力支持是从纳德拉开始的，纳德拉时代的操作系统是windows 10，windows 10之前操作系统是无法使用WSL的，所以下文所有的一切都是在windows 10下的。WSL刚出来的时候还不完善，随着一次一次的更新WSL变得越来越好用，所以使用windows10一定要保持系统日常更新。安装windows 10这里就不介绍了。</p><h2 id="Visual-Studio-Code"><a href="#Visual-Studio-Code" class="headerlink" title="Visual Studio Code"></a>Visual Studio Code</h2><p>这个神器自从发布以来就越来越流行了。可以根据自己的需求随心所欲定制，想让它变成什么IDE就能变成什么IDE。目前我在VS Code上配置的环境能够用于写linux脚本，写markdown文章，写NodeJS程序。现阶段主要用于维护k8s集群。安装的话直接去首页吧[<a href="https://code.visualstudio.com/]" target="_blank" rel="noopener">https://code.visualstudio.com/]</a></p><h2 id="WSL"><a href="#WSL" class="headerlink" title="WSL"></a>WSL</h2><p>很过工程师喜欢在MAC中工作就是因为MAC对linux终端支持是非常棒的，有了WSL之后，就可以在愉快在windows中敲代码，敲命令行，而不需要使用难用的CMD啦！Windows 10 中包含了一个 WSL（Windows Subsystem for Linux）子系统，我们可以在其中运行未经修改过的原生 Linux ELF 可执行文件。利用它我们可以做很多事情，对开发人员和普通用户都是如此。当然对开发人员的吸引力更大一些，因为这意味着在一些情况，不再需要使用 Linux 虚拟机、双系统、Cygwin/MSYS2 了。安装过程在这个网页中[<a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10]，下面就是简单罗列一下：" target="_blank" rel="noopener">https://docs.microsoft.com/en-us/windows/wsl/install-win10]，下面就是简单罗列一下：</a></p><p>以<strong>管理员身份</strong>运行powershell，输入下面的命令：<br><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux</span><br></pre></td></tr></table></figure></p><ul><li>这个步骤之后需要重启，重启之后打开windows store中查找自己要安装的系统，目前支持下面几个：</li><li>Ubuntu</li><li>OpenSUSE</li><li>SLES</li><li>Kali Linux</li><li>Debian GNU/Linux<br>我选择的ubuntu，因为它使用的比较广,而且最关键的是vs code原生支持（vs code的最新版本，我使用的1.22.2是原生支持的）。什么系统确实无所谓的，我们只要用的还是其中的bash。选择好之后就可以安装了，安装过程基本不需要人工干预，一般只需要输入用户名和密码即可。安装完成之后在CMD中输入<code>bash</code>就可以进入WSL的ubuntu 了（不同的系统指令不一样，ubuntu是<code>bash</code>），或者也可从开始菜单中点击ubuntu的进入，如下图：<img src="/2018/04/19/在windows下做系统运维—个人心得/2018-04-19_101617.jpg" title="开始菜单中的ubuntu">进入之后的页面如下：<img src="/2018/04/19/在windows下做系统运维—个人心得/2018-04-19_101745.jpg" title="WSL界面">在vscode中是这样的：<img src="/2018/04/19/在windows下做系统运维—个人心得/2018-04-19_101830.jpg" title="WSL in vscode"></li></ul><p>WSL和windows系统是互不干预的，windows中的磁盘是通过挂载的方式提供给WSL访问的。挂载的位置在<code>/mnt</code>中，其中可以看到我们熟悉的CDE盘。如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">magicsong@W10405:~$ ll /mnt</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 0 root root 4096 Jan 31 08:37 ./</span><br><span class="line">drwxr-xr-x 0 root root 4096 Jan  1  1970 ../</span><br><span class="line">drwxrwxrwx 0 root root 4096 Apr 18 10:40 c/</span><br><span class="line">drwxrwxrwx 0 root root 4096 Apr 18 11:08 d/</span><br><span class="line">drwxrwxrwx 0 root root 4096 Apr 18 16:18 e/</span><br><span class="line">drwxrwxrwx 0 root root 4096 Apr 18 11:03 f/</span><br><span class="line">drwxrwxrwx 0 root root 4096 Apr 18 11:03 g/</span><br></pre></td></tr></table></figure></p><p>虽然很多内核功能无法实现，WSL用于文字处理工作是极其胜任的了，尽情在windows中使用<code>grep</code>，<code>awk</code>和管道处理windows文件吧！😎😎😎😎😎😎😎😎，你会发现用起来非常爽，而且支持auto competition哦！</p><h2 id="配置工作"><a href="#配置工作" class="headerlink" title="配置工作"></a>配置工作</h2><p>安装好必须的软件之后，就需要做一些配置工作。具体用到有下面几个，大家可以参考一下：</p><ol><li>更改默认字体（个人主观需求）。默认字体中的中文显示真是非常不符合我的审美。当然由于控制台比较特殊，它的字体渲染和一般的GUI不一样的，所以改字体的难度很大（大部分中文字体是无法在控制台下使用的，要么用起来极其难看）。不要怕，有大神教——<a href="https://www.zhihu.com/question/36344262/answer/67191917" target="_blank" rel="noopener">为什么 Windows 下 cmd 和 PowerShell 不能方便地自定义字体？ - Belleve的回答 - 知乎</a>，改完之后的bash长这样：<img src="/2018/04/19/在windows下做系统运维—个人心得/2018-04-19_103923.jpg" title="改为后的字体"></li><li>卸载掉所有IDE，文本编辑器（sublime，notepad++等等），除了数据库相关的。</li><li>部署文件同步工具，推荐使用SFTP，在本地测试完成之后，直接SFTP同步到服务器上。我用的是<strong>freefilesync</strong>，可以本地同步，也可以和服务器通过SFTP同步，非常方便。这个工具可以脚本化，从而实现自动同步。</li><li>如果服务器是集群，可以使用NFS的方式，集群之间通过NFS同步代码文件，windows本地可以选择直接挂载NFS文件（这个方式不推荐，用起来不舒服。主要原因还是windows的文件目录系统和linux不一致。linux用到时才会查询文件信息，而windows未使用前就要获取文件信息。NFS出于性能原因时不时会断开，windows每次断开重连之后就要同步，所以会很卡）。</li><li>在vs code安装一些必要的插件，根据自己需要选择安装。这里罗列一些我强烈推荐的：<ul><li>Markdown All in One ，更方便写文章</li><li>Material Icon Theme，给文件添加好看的图标</li><li>Path Intellisense，写代码时方便插入路径</li><li>Bracelet Pair Colorizer，给配对的括号涂上不同的颜色，这样就不会搞混括号啦！</li></ul></li><li>在WSL安装kubectl，安装之后就可以在WSL维护k8s集群了，具体安装参考<a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-via-native-package-management" target="_blank" rel="noopener">在ubuntu中安装kubectl</a>。安装完成之后就可以愉快在WSL中操作k8s了(windows下也是可以用的。但是CMD的操作和linux 差距太大，实在是用不惯，而且没有<code>grep</code>,<code>sed</code>,<code>awk</code>，你能想象一下如何使用！)。如下图：<img src="/2018/04/19/在windows下做系统运维—个人心得/2018-04-19_113507.jpg" title="在WSL中操作k8s"></li></ol><h1 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h1><p>后面如果有什么好的配置会在这里继续更新。<strong>我的目标是一个vs code，一个浏览器，我就能随时随地高效工作！</strong></p>]]></content>
      
      
        <tags>
            
            <tag> WSL </tag>
            
            <tag> vs code </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>深入学习容器——服务暴露</title>
      <link href="/2018/04/17/%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%E5%AE%B9%E5%99%A8%E2%80%94%E2%80%94%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"/>
      <url>/2018/04/17/%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%E5%AE%B9%E5%99%A8%E2%80%94%E2%80%94%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/</url>
      <content type="html"><![CDATA[<blockquote><p>docker环境下，所有的程序都运行在一个独立的空间中（类似于虚拟机），不同容器之间无法互相访问。要想访问容器中的应用，必须要借助于网络。容器能够被外界访问，这就是容器暴露。容器作为服务提供给外界访问，这是服务暴露。由于docker经常以微服务的形式存在，所以本文重点在于服务暴露。当然在介绍服务暴露之前，会介绍容器暴露作为基础。无论怎么样的，访问容器和访问服务都需要网络。<br><a id="more"></a></p></blockquote><h1 id="1-访问容器"><a href="#1-访问容器" class="headerlink" title="1.访问容器"></a>1.访问容器</h1><p>访问容器分两种，一种是机器上的容器访问另一个容器，另外一个是机器之外的其他主机（可以浏览器，也可以是其他应用）访问容器。下面分段介绍。<br><img src="/2018/04/17/深入学习容器——服务暴露/docker-turtles-communication-550x400.jpg" title="容器通信"></p><h2 id="容器之间互访"><a href="#容器之间互访" class="headerlink" title="容器之间互访"></a>容器之间互访</h2><p>容器之间互访又分为两种，一种是同一个机器上的互访，另外一种是不同机器上的互访。同一个机器上的访问需要借助于<strong>bridge</strong>（桥接网络）。所有在本机上创建的容器的都会连接到一个默认的网桥中（网络的名称是bridge）,bridge的一端连着容器的eth0虚拟网卡，另外一端连着主机上的虚拟网络docker0，如下图：<br><img src="/2018/04/17/深入学习容器——服务暴露/two-container-network.png" title="bridge"><br>这个默认的网络主要是用于给容器分配ip地址。但是处于这个默认的bridge中的容器并不能直接通信，想要通信需要使用<strong>自定义网桥</strong>。自定义网桥也是一个虚拟网络，<strong>其本质是设定iptables转发</strong>。原理是根据容器的IP地址的所在域，将外部访问某一个实体网卡端口的流量转发到这个IP上。从上面这段话上，可以知道，容器之间互访，iptables必须要满足两个条件：</p><ol><li>容器之间通信的端口可以访问，本机上不同的容器通信使用的<code>loopback</code>网络，所以和iptables没有关系。但是不同主机之间的容器互访需要这个端口打开。具体的端口通常是随机的一个大号端口，通常设定两台机器互访无限制即可。</li><li>iptables 转发要打开，具体打开需要下面两个语句：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#打开IPv4转发功能</span></span><br><span class="line">sysctl net.ipv4.conf.all.forwarding=1</span><br><span class="line"><span class="comment">#打开防火墙转发策略</span></span><br><span class="line">sudo iptables -P FORWARD ACCEPT</span><br><span class="line"><span class="comment">#上面这两个语句都不是永久的，永久的需要写入具体的配置文件。一般在安装集群的时候都会要求上面两个永久写入配置文件。请参考docker集群的安装</span></span><br></pre></td></tr></table></figure></li></ol><p>还需要说的一点，由于容器的生命周期较短，经常会重启，所以它的IP地址不是固定的，如果想要直接访问容器中的应用，可以通过下面的语句获取ip地址：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker inspect <span class="variable">$CONTAINER_ID</span> | grep IPAddress</span><br></pre></td></tr></table></figure></p><blockquote><p>由于IP的不固定性（但是docker的ip不是随机的，而是按照一定顺序使用的），通常需要用到DNS，即创建容器时就将容器的IP地址写入DNS，那样的话就可以用过DNS中设定的名称访问容器了。具体和DNS配置的信息我还没有研究过，都是用docker现成的，后续如果需要用到可以研究一下。</p></blockquote><p>上面讲的配置微微有点偏题，在同一个主机上的不同容器互访没有作用（但是对下文很有作用，这里就提前打好基础。兵马未动粮草先行，😃）。下面介绍如何让同一个机器上的两个容器通信。</p><h2 id="同一台主机上的容器通信"><a href="#同一台主机上的容器通信" class="headerlink" title="同一台主机上的容器通信"></a>同一台主机上的容器通信</h2><p>docker官方提供了一个方案，就是将多个容器编入同一个docker-compose，这样的话docker会为这个compose创建一个默认的自定义网络，处于这个自定义网络中的容器是互相访问没有限制的。并且这个自定义网络中的容器之间都互相配置了DNS解析，可以直接使用容器的名称代替IP地址访问。下面演示一下docker-compose。<br>首先下面是我写好的一份docker-compose文件，保存为docker-compose.yml即可。<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">"3"</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"><span class="attr">  portainer-web:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">"rdsource.tp-link.net:8088/portainer/portainer"</span></span><br><span class="line"><span class="attr">    command:</span> <span class="bullet">--templates</span> <span class="attr">http://portainer-templates/templates.json</span></span><br><span class="line"><span class="attr">    restart:</span> <span class="string">on-failure</span></span><br><span class="line"><span class="attr">    ports:</span> </span><br><span class="line"><span class="bullet">      -</span> <span class="string">"9000:9000"</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"/opt/docker_data/portainer:/data"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"/var/run/docker.sock:/var/run/docker.sock"</span></span><br><span class="line"><span class="attr">    networks:</span> </span><br><span class="line"><span class="bullet">      -</span> <span class="string">portainer_net</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  portainer-templates:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">"portainer-templates"</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"/opt/docker_data/portainer_template/templates.json:/usr/share/nginx/html/templates.json"</span> </span><br><span class="line"><span class="attr">    expose:</span> </span><br><span class="line"><span class="bullet">      -</span> <span class="string">"80"</span></span><br><span class="line"><span class="attr">    restart:</span> <span class="string">on-failure</span></span><br><span class="line"><span class="attr">    networks:</span> </span><br><span class="line"><span class="bullet">      -</span> <span class="string">portainer_net</span></span><br><span class="line">      </span><br><span class="line"><span class="attr">networks:</span></span><br><span class="line"><span class="attr">  portainer_net:</span></span><br><span class="line"><span class="attr">    driver:</span> <span class="string">bridge</span></span><br><span class="line"><span class="attr">    ipam:</span></span><br><span class="line"><span class="attr">      config:</span></span><br><span class="line"><span class="attr">       - subnet:</span> <span class="number">172.81</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br></pre></td></tr></table></figure></p><p>上面这个compose创建了两个应用，一个web应用portainer-web，应用的端口是9000，一个用于web应用的静态资源portainer-template（用nginx做代理），可以看到portainer-web访问portainer-template的80端口是直接使用它的名称而不是IP地址。同时这个compose还在最后创建了一个自定义网络，平时不需要手动创建，这次只是为了演示。上面两个应用都通过<code>networks</code>关键词加入了这个网络（为什么是networks呢？因为一个容器理所当然可以加入很多网络啦。😆）。在docker-compose文件所在目录中执行<code>docker-compose up -d</code>(<code>-d</code> 是表示后台运行)。这样docker就会启动了上述两个应用，并且还有一个叫portainer_net的bridge网络。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@itdocker portainer]<span class="comment"># docker-compose up -d</span></span><br><span class="line">Creating network <span class="string">"portainer_portainer_net"</span> with driver <span class="string">"bridge"</span></span><br><span class="line">Creating portainer_portainer-web_1       ... <span class="keyword">done</span></span><br><span class="line">Creating portainer_portainer-templates_1 ... <span class="keyword">done</span></span><br></pre></td></tr></table></figure></p><p>下面进入portainer-templates容器中看看是否可以访问portainer-web。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/ # ping portainer-web</span><br><span class="line">PING portainer-web (172.81.0.2): 56 data bytes</span><br><span class="line">64 bytes from 172.81.0.2: seq=0 ttl=64 time=0.197 ms</span><br><span class="line">64 bytes from 172.81.0.2: seq=1 ttl=64 time=0.132 ms</span><br><span class="line">64 bytes from 172.81.0.2: seq=2 ttl=64 time=0.182 ms</span><br><span class="line">64 bytes from 172.81.0.2: seq=3 ttl=64 time=0.186 ms</span><br><span class="line">64 bytes from 172.81.0.2: seq=4 ttl=64 time=0.175 ms</span><br><span class="line">64 bytes from 172.81.0.2: seq=5 ttl=64 time=0.131 ms</span><br><span class="line">^C</span><br><span class="line">--- portainer-web ping statistics ---</span><br><span class="line">6 packets transmitted, 6 packets received, 0% packet loss</span><br></pre></td></tr></table></figure></p><p>可以看到已经ping通了。<br>用wget测试一下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/ <span class="comment"># wget portainer-web:9000</span></span><br><span class="line">Connecting to portainer-web:9000 (172.81.0.2:9000)</span><br><span class="line">index.html           100% |********************************************************************************************************************************************|  2748   0:00:00 ETA</span><br></pre></td></tr></table></figure></p><p>wget也成功了。这就说明两个容器之间的互访已经没有问题了。<br>这是docker-compose的方式（k8s集群中也提供了类似的方式），也是docker提供的同一个主机中容器间互访的解决方案，步骤主要是创建一个bridge网络，然后添加DNS解析，实现容器间互访。</p><h2 id="不同主机的容器（服务）互访"><a href="#不同主机的容器（服务）互访" class="headerlink" title="不同主机的容器（服务）互访"></a>不同主机的容器（服务）互访</h2><p>这里说的不同主机间的容器互访指的是同一个集群中的不同的主机。不在同一个集群中主机中容器互访是通过主机暴露端口实现的，下文会讲到。同一个集群中的不同主机的容器互访要解决的问题主要有两个：</p><ol><li>服务发现。容器在集群中可能动态分布到不同主机中，ip也不固定，需要有一个中间件管理这个服务分布信息</li><li>要有一个网络，无论其容器分布在哪个主机，哪个IP，其在这个网络上暴露的IP不会变，不然很有可能其他容器无法访问这个容器。</li></ol><p>在集群中实现容器互访，docker提供了一种<strong>overlay</strong>网络，这个overlay网络是基于<strong>swarm</strong>集群的。docker swarm集群是docker原生的集群管理工具，和docker集成度很高，所以用起来非常方便。overlay的架构图如下：<br><img src="/2018/04/17/深入学习容器——服务暴露/1nNoIXGkJiDax7l5g5GxH7nTzqrqzN7Y9aBZTaXoQ8Q=.png" title="overlay网络"><br>overlay网络是受Docker Universal Control Plane （UCP）控制的，UCP负责集群中不同主机中容器管理。UCP将跨主机的容器作为<strong>服务</strong>（service），服务发现也由UCP实现。但是这个原生的服务发现在一些简单的环境中还可以，但是一旦到了大型集群中，能力就有点捉襟见肘了（主要原因还是起步太晚，尚未成熟）。工业界比较成熟的方案是<strong>Etcd</strong>，k8s默认就是采用了这个方案。swarm在小型环境下还不错，并且支持windows，对于一些企业的小型架构来说已经够了。<br>一旦创建了一个swarm集群，那么docker就会有默认的overlay网络，叫<strong>ingress</strong>，可以自定义这个ingress网络，如修改其网域和网关等等。每一个主机都通过一个bridge和这个overlay网络连接。处于overlay网络中的服务默认是<strong>负载均衡</strong>的。即如果一个服务有多个容器，那么流量会均摊到这些容器上，采用轮询的方式选择容器（round robin）。在docker swarm中创建一个服务（只能创建服务，创建单个容器不要集群）会指定<strong>replica</strong>，表明这个服务的后端有多少个进程，这些进程以容器的形式随机分布在各个节点上。那么其他容器如何访问这个服务呢？通过<strong>服务发现</strong>，发布服务时，会将服务随机绑定在overlay网络上的某个端口上，一旦检测到这个端口的流量，就会在UCP中找到服务对应的容器，获取它的IP地址和暴露的端口（服务的容器在哪个节点，Ip是多少都是注册在UCP中），UCP将这个流量采用轮询的方式转发到对应容器中。这样一来，容器之间就能够通讯了，这是外界访问这个服务也是同样的原理。在外界看来，服务是暴露的端口，只要访问集群中任意一个节点的某个端口，就能实现访问服务。服务之间互访需要一个自定义的overlay网络而不是用默认的overlay（通过docker stack部署会自动创建），这样不同的服务之间就能互相访问了。某一个主机单单的容器只要和加入这个网络，也能实现互访，不然只能通过其暴露的端口进行访问。<br>在k8s中还要这个稍微复杂一点，这点以后再讲。</p><h1 id="2-外部访问容器"><a href="#2-外部访问容器" class="headerlink" title="2.外部访问容器"></a>2.外部访问容器</h1><p>稍微学习过docker的都知道访问容器是通过容器暴露端口，并将端口映射到主机端口上。外部通过主机ip+端口进行访问。对于服务也是一样的，虽然服务的容器是随机分布到一些节点中，但是通过任意节点+端口都能访问这个应用，无论这个节点上是否有对应容器（同理，就算指定了节点+端口，也不一定能访问这个节点的容器），具体访问哪个节点是轮询的（对于用户来说基本上就是随机的了）。<br>这种情况下，如果docker上有上百个服务，那么用户访问特定应用就需要记忆特定的端口，长期以往应用管理也非常不方便。用户反映某个应用不能访问，告诉管理员端口，然后管理员还要查询这个端口对应哪个容器。天哪。😢。docker swarm 没有提供关于这个问题的解决方案，而相对成熟的k8s使用nginx解决了这个问题，使得用户可以使用子域名访问不同的应用。在这个模式下，所有的web应用都通过主机上的80端口，监控80端口的nginx根据访问的域名将请求转发到后台的服务中，如何实现将在下一篇文章中写。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>docker简化了访问容器和互相访问的操作，使得几个命令就可以实现外部和内部访问特定容器和服务。这个模式在小规模集群中非常好用，但是一旦涉及到大的集群，涉及到集群分类等等，swarm就有心无力了。在这种情况下，k8s是更好的选择。同时k8s还提供了一整套集群监控管理方案，相对于swarm更加成熟。小打小闹适合用swarm，上手极快。生产环境中建议用k8s。</p>]]></content>
      
      
        <tags>
            
            <tag> network </tag>
            
            <tag> service </tag>
            
            <tag> kubernetes </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>学习在集群中工作—NFS文件共享</title>
      <link href="/2018/04/13/%E5%AD%A6%E4%B9%A0%E5%9C%A8%E9%9B%86%E7%BE%A4%E4%B8%AD%E5%B7%A5%E4%BD%9C%E2%80%94NFS%E6%96%87%E4%BB%B6%E5%85%B1%E4%BA%AB/"/>
      <url>/2018/04/13/%E5%AD%A6%E4%B9%A0%E5%9C%A8%E9%9B%86%E7%BE%A4%E4%B8%AD%E5%B7%A5%E4%BD%9C%E2%80%94NFS%E6%96%87%E4%BB%B6%E5%85%B1%E4%BA%AB/</url>
      <content type="html"><![CDATA[<blockquote><p>刚进公司的时候第一次接触的服务器就是一个集群，一个RAC集群，为了集群正常工作，有一些文件需要在多个节点上一致，公司一些OA系统往往也需要所有节点的文件一致。以前采用的lsyncd+rsync的同步的方式进行工作，但是很容易出错，主要原因就是双相同步容易冲突，在高并发的时候就会出现异常。在学习docker集群的过程中也遇到了这些问题。由于一些服务需要在配置高可用（同时也是负载均衡），这些服务需要访问同一个文件目录。NFS能够解决这种文件共享问题。</p></blockquote><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>NFS 是Network File System的缩写，即网络文件系统。功能是通过网络让不同的机器、不同的操作系统能够彼此分享文件，让应用程序在客户端通过网络访问位于服务器磁盘中的数据，是在类Unix系统间实现磁盘文件共享的一种方法。NFS使用RPC协议进行通信，也就是说NFS系统只是一组RPC程序。RPC是远程过程调用 (Remote Procedure Call) 的英文缩写，它是能使客户端执行其他系统中程序的一种机制。NFS可以看作是一个RPC Server，主要功能是管理需要分享的目录和文件。它不负责通信和信息传输，而是把这部分工作交给RPC协议来完成。即NFS在文件传送或信息传送过程中依赖于RPC协议。所以只要用到NFS的地方都要启动RPC服务，不论是NFS SERVER或者NFS CLIENT。这样SERVER和CLIENT才能通过RPC来实现PROGRAM PORT的对应。可以这么理解RPC和NFS的关系：NFS是一个文件系统，而RPC是负责负责信息的传输。<br><a id="more"></a><br>NFS 的基本原则是“容许不同的客户端及服务端通过一组RPC分享相同的文件系统”，它是独立于操作系统，容许不同硬件及操作系统的系统共同进行文件的分享。</p><h1 id="2-搭建NFS文件服务器"><a href="#2-搭建NFS文件服务器" class="headerlink" title="2.搭建NFS文件服务器"></a>2.搭建NFS文件服务器</h1><h2 id="安装软件"><a href="#安装软件" class="headerlink" title="安装软件"></a>安装软件</h2><p>搭建NFS非常简单，本文的实验环境是centos 7.4，搭建了NFS文件服务器之后，只要有支持NFS的软件，任何系统都可以挂载NFS服务器挂载的目录。本次安装关闭了防火墙，如果开启了防火墙的话，需要打开对应端口。<br>执行下面的命令安装所需的软件：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install -y nfs-utils</span><br><span class="line">systemctl <span class="built_in">enable</span> rpcbind &amp;&amp; systemctl start rpcbind</span><br><span class="line">systemctl <span class="built_in">enable</span> nfs &amp;&amp; systemctl start nfs</span><br></pre></td></tr></table></figure></p><p>上述命令会把NFS的依赖rpcbind也装上。安装之后直接启动NFS就可以了。</p><h2 id="配置NFS"><a href="#配置NFS" class="headerlink" title="配置NFS"></a>配置NFS</h2><p>NFS的配置文件为<code>/etc/exports</code>，如果不存在这个文件，自己建一个即可。下面贴上我的配置：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/opt/docker_files 172.31.73.76(rw,no_root_squash,async) 192.168.22.0/24(rw,no_root_squash)</span><br><span class="line">/opt/docker_data  192.168.22.0/24(rw,no_root_squash)</span><br><span class="line">/opt/softwares 192.168.22.0/24(rw,no_root_squash)</span><br></pre></td></tr></table></figure></p><p>配置文件非常简单，一行一个目录，后面跟上可以访问的地址即可，多个ip地址之间通过空格分开，ip地址可以是单独的ip（<code>172.31.73.76</code>是我的ip），也可以是一个域（<code>192.168.22.0/24</code>这个域包含了集群中的所有节点）。或者也可以用域名通配符，比如<code>*.rd.tp-link.net</code>。地址后面括号表示了访问权限以及如何处理访问的用户。rw表示可读可写，no_root_squash表示将所有使用这个目录的人当做root。使用这个目录的人就是当前挂载这个目录的主机正在使用这个文件夹的用户。上述配置在公网中是非常不安全的，它允许客户端root访问root文件夹，但是客户端如果不是root的话是不具备root权限的，客户端所有非root都会被当成other。</p><p>上面的选项中有个async，它表示文件是异步写入NFS服务器的，这里之所以这么配置是因为windows的文件浏览器特别不好使，它在打开一个文件夹时会读取很多信息，导致在windows上挂载一个NFS文件夹使用起来非常难受。异步的话就可以将写入延后，稍微提升一些流畅性。默认是 Sync，表示必须写入NFS服务器，写的操作才能成功返回。</p><p>上面的配置完成之后，执行<code>exportfs -arv</code> 重新加载NFS目录，输出如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@itdocker influxdb]<span class="comment"># exportfs -arv</span></span><br><span class="line">exporting 172.31.73.76:/opt/docker_files</span><br><span class="line">exporting 192.168.22.0/24:/opt/softwares</span><br><span class="line">exporting 192.168.22.0/24:/opt/docker_data</span><br><span class="line">exporting 192.168.22.0/24:/opt/docker_files</span><br></pre></td></tr></table></figure></p><p>可以在本地看看有哪些可用的NFS文件夹，命令是<code>showmount -e localhost</code>，输出如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@itdocker influxdb]<span class="comment"># showmount -e localhost</span></span><br><span class="line">Export list <span class="keyword">for</span> localhost:</span><br><span class="line">/opt/softwares    192.168.22.0/24</span><br><span class="line">/opt/docker_data  192.168.22.0/24</span><br><span class="line">/opt/docker_files 192.168.22.0/24,172.31.73.76</span><br></pre></td></tr></table></figure></p><p>一般配置没什么问题就能有上面的输出。</p><p>##客户端配置——linux<br>客户端只需安装rpcbind就可以了，安装完成之后，同样使用<code>showmount -e x.x.x.x</code>（NFS ip）查询有哪些可以挂载的文件夹。挂载的过程和挂载硬盘相似，首先创建一个挂载点，然后将NFS上的文件夹挂载在这个点上。参考的命令如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir /opt/docker_files</span><br><span class="line">mount 172.29.41.127:/opt/docker_files  /opt/docker_files</span><br></pre></td></tr></table></figure></p><p>这样就挂载完毕了，可以随意创建一个文件实验一下，看一下NFS服务器和本地是否一致。</p><p>##客户端配置——windows<br>如果想要在windows上操作NFS文件，步骤稍微有一些麻烦。首先要开启NFS Client的功能，如下图<br><img src="/2018/04/13/学习在集群中工作—NFS文件共享/2018-04-13_174116.jpg" title="在windows中开启NFS Client"><br>开启完毕之后就可以使用形如右边的命令将NFS共享文件夹挂载在一个新的卷标上<code>mount 172.29.41.127:/opt/docker_files x:</code>。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">C:\Users\admin&gt;mount 172.29.41.127:/opt/docker_files x:</span><br><span class="line">x: is now successfully connected to 172.29.41.127:/opt/docker_files</span><br><span class="line"></span><br><span class="line">The <span class="built_in">command</span> completed successfully.</span><br></pre></td></tr></table></figure></p><p>这个命令是将/opt/docker_files的作为本地x盘使用，如下图：<br><img src="/2018/04/13/学习在集群中工作—NFS文件共享/2018-04-13_174638.jpg" title="windows挂载"></p><p>如果NFS服务器共享的文件是root文件，那么windows上将无法写入，因为windows使用这个目录时的身份不是root（windows没有和linux一样的用户机制）。为此要么将共享文件夹变成777，或者更改注册表。<br>解决办法就是让Win7在挂载NFS的时候将UID和GID改成0即可：打开注册表：HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\ClientForNFS\CurrentVersion\Default，增加两项：AnonymousUid，AnonymousGid,如图：<br><img src="/2018/04/13/学习在集群中工作—NFS文件共享/2018-04-13_180050.jpg" title="修改注册表"><br>这样修改就会使windows用户以root身份登录，然后就可以有写的权限。</p><h1 id="3-NFS使用的陷阱"><a href="#3-NFS使用的陷阱" class="headerlink" title="3.NFS使用的陷阱"></a>3.NFS使用的陷阱</h1><ol><li>[!!!重要]如果客户端没有断开连接，NFS服务器是关不了机的，除非kill掉rpcbind和NFS进程。所以常常会给NFS客户端配置autofs，autofs会在不使用这个目录的时候自动取消挂载。</li><li>linux 挂载一定记得设置开机挂载，在/etc/rc.d/rc.local中设置就行</li><li>一定要注意NFS的版本，V2和V3是不支持文件锁的，必须手动在服务器端和客户端开启nfslock才行。而V4是内置的。</li><li>v2-3默认是异步写，而v4默认是同步写，所以一定要注意NFS版本。在客户端df -hT查看文件系统就能知道是V4 还是V2-3，V4显示为<code>NFS4</code>。一般来说centos6 系列是3，而centos7 系统是4。</li><li>如果NFS服务器挂掉了，而客户端尝试使用这个挂载目录的话，会将命令直接卡死，所以NFS服务器挂掉的话对客户端影响很大。 </li></ol>]]></content>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> NFS </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>离线安装最新版kubernetes（1.10）</title>
      <link href="/2018/04/08/%E7%A7%91%E5%AD%A6%E5%AE%89%E8%A3%85%E6%9C%80%E6%96%B0%E7%89%88kubernetes/"/>
      <url>/2018/04/08/%E7%A7%91%E5%AD%A6%E5%AE%89%E8%A3%85%E6%9C%80%E6%96%B0%E7%89%88kubernetes/</url>
      <content type="html"><![CDATA[<blockquote><p>随着docker的流行，越来越多企业采用了容器技术作为IT部门的基础架构，越来越多的运用在了生产环境中。生产环境中各个应用都是以微服务的形式分布在各个主机中，k8s就是这样用于在分布式系统中管理容器的一种编排工具，提供基本<br>的部署，维护以及运用伸缩。<br> <a id="more"></a></p></blockquote><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p><strong>Kubernetes </strong>是 Google 团队发起的开源项目，它的目标是管理跨多个主机的容器，主要实现语言为 Go 语言。Kubernetes 构建于 Google 数十年经验，一大半来源于 Google 生产环境规模的经验。结合了社区最佳的想法和实践。在分布式系统中，部署，调度，伸缩一直是最为重要的也最为基础的功能。Kubernets 就是希望解决这一序列问题的。Kubernetes 有如下这些特点：</p><ul><li>易学：轻量级，简单，容易理解</li><li>便携：支持公有云，私有云，混合云，以及多种云平台</li><li>可拓展：模块化，可插拔，支持钩子，可任意组合</li><li>自修复：自动重调度，自动重启，自动复制</li></ul><p>也是由于这些特点，k8s已经成为大多数企业的容器编排工具，而docker官方的<strong>docker swarm</strong> 则由于后知后觉的缘故而使用较少（docker swarm是最新版本的docker自带的，无需安装，使用简单，初学容器编排时可以尝试一下）。</p><blockquote><p>学习k8s有一定的难度，本文着重于介绍如何搭建一个可用的k8s集群，后续会写一些文章重点学习k8s的一些key concept。</p></blockquote><h1 id="2-安装Kubernetes"><a href="#2-安装Kubernetes" class="headerlink" title="2. 安装Kubernetes"></a>2. 安装Kubernetes</h1><p>k8s官方文档中介绍了很多种部署k8s集群的方式，有单机实验性质的，本地多机部署方式，也有云端部署的方式，本文介绍的是第二种<strong>本地多机部署</strong>——即利用本地多台机器部署一个k8s集群，其他方式可以参看<a href="https://kubernetes.io/docs/home/" target="_blank" rel="noopener">官方文档</a>。需要注意的是，官方文档提供的部署方式由于一些不可描述原因在我国实施起来有一定难度，所以本文的安装需要<strong>科学上网</strong>。并且，本次实验的三台主机都是在局域网中，没有联机权限，所以并不要求服务器能够科学上网。<br>在我安装的时候，最新版的k8s的版本是<code>1.10</code>，和其他版本的安装过程是相似的。</p><h2 id="Prerequisites-检查"><a href="#Prerequisites-检查" class="headerlink" title="Prerequisites 检查"></a>Prerequisites 检查</h2><p>在参照本文部署最新版k8s前，请一定要注意k8s安装的前置条件，现在罗列如下，请依依对照，一条不可漏下。</p><ol><li>有一台能够科学上网的机器，下载相关的软件和镜像</li><li>下列操作系统之一<ul><li>Ubuntu 16.04+</li><li>Debian 9</li><li>CentOS 7</li><li>RHEL 7</li><li>Fedora 25/26 (best-effort)</li><li>HypriotOS v1.0.1+</li><li>Container Linux (tested with 1576.4.0)</li></ul></li><li>每台主机至少2G内存，双核</li><li>集群中所有主机互相访问没有限制（无论是公网还是私有网卡）<strong>UPDATE</strong>，这里我踩了一个超级大坑，我司申请的两台机器互相访问无限制往往是TCP端口无限制，但是k8s需要用到UDP。所以一定要确保节点之间UDP访问也无限制！</li><li>主机名，MAC地址，产品UUID必须不同（一般都是不同的，除非是用的是复制的虚拟机）</li><li>如果使用了防火墙，必须打开一些指定端口（下文会讲到，如果嫌麻烦，直接关了防火墙，本次实验没有关闭防火墙）</li><li>禁用SWAP内存。可以使用命令<code>swapoff -a</code>，其他方式也可以。</li></ol><p>下面列出安装k8s会使用到端口，如果使用防火墙的话，请将这些端口打开（我在实验时遇到了一个坑就是端口问题，我在防火墙配置了三个节点互相访问没有限制，但是这还不是不够的，一些容器是通过虚拟网卡访问接口的，所以还是需要打开一些额外的端口。这里事先记录一下，如果在安装过程中打开了防火墙，并且也配置了三个节点ip互相访问没有限制，那么还需要再打开的默认端口——在不修改默认端口的前提下——6443，10250-10252，请确保这些端口对公可访问，<strong>在初始化的时候会提示你打开这些端口</strong>）。<br><strong>master节点</strong></p><table><thead><tr><th>协议</th><th>方向</th><th>端口</th><th>目的</th></tr></thead><tbody><tr><td>TCP</td><td>Inbound</td><td>6443</td><td>k8s api 服务器</td></tr><tr><td>TCP</td><td>Inbound</td><td>2379-2380</td><td>etcd 客户端API</td></tr><tr><td>TCP</td><td>Inbound</td><td>10250</td><td>Kubelet API</td></tr><tr><td>TCP</td><td>Inbound</td><td>10251</td><td>kube-scheduler</td></tr><tr><td>TCP</td><td>Inbound</td><td>10252</td><td>kube-controller-manager</td></tr><tr><td>TCP</td><td>Inbound</td><td>10255</td><td>Read-only Kubelet API</td></tr></tbody></table><p><strong>worker节点</strong></p><table><thead><tr><th>协议</th><th>方向</th><th>端口</th><th>目的</th></tr></thead><tbody><tr><td>TCP</td><td>Inbound</td><td>10250</td><td>Kubelet API</td></tr><tr><td>TCP</td><td>Inbound</td><td>10255</td><td>Read-only Kubelet API</td></tr><tr><td>TCP</td><td>Inbound</td><td>30000-32767</td><td>服务端口</td></tr></tbody></table><p><em>如果需要修改一些默认端口，那么需要确保修改后的端口也打开。</em></p><h2 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h2><p>docker 的安装可以参考此文：<a href="http://itdocker.rd.tp-link.net:9002/2018/03/14/Docker%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97/" target="_blank" rel="noopener">docker部署指南</a><br>如果是运行在公网中的服务器，请查看官方文档。</p><h2 id="安装kudeadm"><a href="#安装kudeadm" class="headerlink" title="安装kudeadm"></a>安装kudeadm</h2><p>本次实验使用了三台centos7.4的主机，安装的docker使用了最新的docker-ce。</p><p>kubeadm 是google公司用来简化k8s集群搭建的一个工具，安装完kubeadm之后，只需要一行命令就可以初始化一个集群的master节点，非常方便。安装kubeadm的时候，还需要安装kubelet和kubectl，这两个工具是用来管理集群的工具，kubeadm本身不会安装这两个工具。需要注意kubeadm和这两工具的版本需要一致，此次的版本都是1.10。由于服务器不能上网，此次我们需要手动安装rpm包。在一台能科学上网的机器上输入以下网址（这个网址其实就是google提供的yum源中软件的安装位置）：<a href="https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64/repodata/primary.xml" target="_blank" rel="noopener">https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64/repodata/primary.xml</a><br>这个xml文件中包括了所有需要的rpm包和其依赖项，本次安装需要用到下面4个包：</p><ul><li>kubeadm</li><li>kubectl </li><li>kubelet</li><li>kubernetes-cni </li></ul><p>请在上面的xml找到上面软件的最新版（大部分是1.10，如果不是，请确保是在上述xml文件中是最新的），复制其对应的地址进行下载，下载完成之后将rpm包传输到所有服务器节点即可。本次下载的截图如下：<br><img src="/2018/04/08/科学安装最新版kubernetes/2018-04-08_114922.jpg" title="所需的安装包"><br>下载完成之后，在所有节点安装这些程序：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">setenforce 0</span><br><span class="line">yum install -y *.rpm</span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet &amp;&amp; systemctl start kubelet</span><br></pre></td></tr></table></figure></p><h2 id="后续配置"><a href="#后续配置" class="headerlink" title="后续配置"></a>后续配置</h2><blockquote><p>安装完成之后需要进行一系列和k8s配置相关的后续操作。有一些操作需要在所有节点进行，而有一些只需要在master 节点进行。</p></blockquote><p>需要注意<code>setenforce 0</code>，目前k8s的大部分网络插件都无法在selinux下运行，所以需要将selinux关闭。如果在安装网络插件的过程中遇到问题。可以检查一下是否是这个问题（当初就是踩了这个坑）。另外如果和我一样操作系统也是centos7，那么还需要进行下面的配置：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br></pre></td></tr></table></figure></p><p>请注意。上述所有操作在<strong>所有节点</strong>都要进行。<strong>网络插件的安装我遇到了不少问题，这部分大家可以参考我的，一步一步谨慎操作。</strong><br>除了网络配置，还需要检查主节点k8s cgroup driver是否和docker 的cgroup driver相同，如果不相同需要相同。这一步只需要在主节点完成。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#检查下面两个命令的输出</span></span><br><span class="line">docker info | grep -i cgroup</span><br><span class="line">cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#如果不相同，需要将k8s的配置改为和docker一致，执行下面的命令</span></span><br><span class="line">sed -i <span class="string">"s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g"</span> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br></pre></td></tr></table></figure></p><p>上述后续配置都完成之后，重启kubelet<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure></p><p>如果没出现什么问题，那么kubeadm的安装已经完成，下面就需要用kubeadm创建一个集群。</p><h2 id="用kubeadm创建一个集群"><a href="#用kubeadm创建一个集群" class="headerlink" title="用kubeadm创建一个集群"></a>用kubeadm创建一个集群</h2><p><strong>1. 阅读相关网络插件说明</strong></p><p>kubeadm 极大地简化了创建一个k8s集群的步骤，但它本身没有能力提供集群的网络解决方案。我们需要选择一个<strong>合适</strong>的网络解决方案。在熟悉k8s之前，我们无法判断一个网络解决方案到底合不合适，所以本次实验选择了一个比较容易上手的网络插件——flannel。不同的网络插件下，k8s初始化的步骤不同，请一定要参考相关的文档进行设置。</p><p><strong>2. 手动拉取一些基础镜像</strong></p><p>kubeadm在初始化的过程中会启动一系列docker镜像（而且是墙外的镜像），由于我们的服务器无法联网，我们需要在一个能够科学上网的机器上安装docker，并拉取相关的镜像。需要拉取的镜像列表如下，我用的tag是latest，目前没有遇到什么问题：</p><ul><li>k8s.gcr.io/kube-proxy-amd64</li><li>k8s.gcr.io/kube-controller-manager-amd64</li><li>k8s.gcr.io/kube-apiserver-amd64</li><li>k8s.gcr.io/kube-scheduler-amd64</li><li>k8s.gcr.io/etcd-amd64</li><li>k8s.gcr.io/kubernetes-dashboard-amd64</li><li>k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64</li><li>k8s.gcr.io/k8s-dns-sidecar-amd64</li><li>k8s.gcr.io/k8s-dns-kube-dns-amd64</li><li>k8s.gcr.io/pause-amd64</li></ul><p>除了上述基础镜像，还需要拉取网络插件镜像，取决于所采用的网络方案，例如本次下载的镜像是：quay.io/coreos/flannel。如何知道k8s需要哪些镜像呢？其实如果不事先拉取镜像，直接运行<code>kubeadm init</code>，会在<code>/etc/kubernetes/manifests/</code>目录下生成一系列yml文件，这些yml文件告知了k8s在启动过程中需要哪些镜像（如果一个一个去看了的话，你会发现上述有一个镜像并不是初始化集群必须的。哈哈，不不卖关子了，就是那个dashboard，这只是一个管理k8s的web ui）。<br>如果你没有拉取镜像，而直接运行了<code>kubeadm init</code>，肯定会报错，报错之后如果想要重新安装，请一定要执行<code>kubeadm reset</code></p><p>拉取完上述镜像之后，将上述镜像传输到主节点中。还有一部分镜像需要传输到从节点中，列表如下：</p><pre><code>* k8s.gcr.io/kube-proxy-amd64* k8s.gcr.io/kubernetes-dashboard-amd64* k8s.gcr.io/pause-amd64* quay.io/coreos/flannel</code></pre><p>在局域网中，镜像共享需要私有镜像。传输镜像的步骤为，tag镜像，然后push到私有仓库，或者也可以采用下面的方式直接传输，避免需要不断重复tag。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker save k8s.gcr.io/pause-amd64:3.1 | bzip2 | pv | ssh root@nodeip <span class="string">'cat | docker load'</span></span><br><span class="line"><span class="comment">#pv可以将进度可视化，需要安装一下，没有pv也一样OK</span></span><br></pre></td></tr></table></figure></p><p><strong>3. 初始化集群</strong></p><p>首先在主节点运行下面的命令初始化为一个集群：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeadm init --kubernetes-version=v1.10.0 --pod-network-cidr=10.244.0.0/16</span><br></pre></td></tr></table></figure></p><p><code>kubeadm init</code> 用于初始化一个集群，后面第一参数是显示告知集群的版本，如果不设置kubeadm会去网络上下载相关的版本信息，由于我们的服务器无法联网，这一步就会出错。所以一定要添加这个命令。第二个参数是flannel相关的参数设置，其中的地址是固定的10.244.0.0/16，这是在flannel配置文件中写死的。<br>上述命令的输出很重要，大部分应该下面这样的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[init] Using Kubernetes version: v1.8.0</span><br><span class="line">[init] Using Authorization modes: [Node RBAC]</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)</span><br><span class="line">[certificates] Generated ca certificate and key.</span><br><span class="line">[certificates] Generated apiserver certificate and key.</span><br><span class="line">[certificates] apiserver serving cert is signed for DNS names [kubeadm-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.138.0.4]</span><br><span class="line">[certificates] Generated apiserver-kubelet-client certificate and key.</span><br><span class="line">[certificates] Generated sa key and public key.</span><br><span class="line">[certificates] Generated front-proxy-ca certificate and key.</span><br><span class="line">[certificates] Generated front-proxy-client certificate and key.</span><br><span class="line">[certificates] Valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: &quot;admin.conf&quot;</span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: &quot;kubelet.conf&quot;</span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: &quot;controller-manager.conf&quot;</span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: &quot;scheduler.conf&quot;</span><br><span class="line">[controlplane] Wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;</span><br><span class="line">[controlplane] Wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;</span><br><span class="line">[controlplane] Wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;</span><br><span class="line">[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;</span><br><span class="line">[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[init] This often takes around a minute; or longer if the control plane images have to be pulled.</span><br><span class="line">[apiclient] All control plane components are healthy after 39.511972 seconds</span><br><span class="line">[uploadconfig] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[markmaster] Will mark node master as master by adding a label and a taint</span><br><span class="line">[markmaster] Master master tainted and labelled with key/value: node-role.kubernetes.io/master=&quot;&quot;</span><br><span class="line">[bootstraptoken] Using token: &lt;token&gt;</span><br><span class="line">[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class="line">[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstraptoken] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[addons] Applied essential addon: kube-dns</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes master has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run (as a regular user):</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  http://kubernetes.io/docs/admin/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of machines by running the following on each node</span><br><span class="line">as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join --token &lt;token&gt; &lt;master-ip&gt;:&lt;master-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;</span><br></pre></td></tr></table></figure></p><p>上述输出告知了后续的操作，其中最重要的就是最后的token那段，那是节点加入整个集群的关键语句，需要保存以备后续节点加入使用，如果忘记了话可以用<code>kubeadm token</code>命令进行查询。上述命令成功的话不代表集群创建成功了，后续还需要安装网络插件。</p><p>（可选）如果需要以非root的身份使用k8s，那么需要切换到其用户下，执行下面的命令（该用户需要有sudo权限）：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br></pre></td></tr></table></figure></p><p>如果是root用户，只需要在<code>.bash_profile</code>中添加下面一句即可。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> KUBECONFIG=/etc/kubernetes/admin.conf</span><br></pre></td></tr></table></figure></p><p><strong>4. 安装网络插件</strong></p><p><em>网络插件</em> 这一块是我踩坑的密集区，大部分都是和上面的讲到的步骤有关。上述讲到的步骤漏了一步都会导致网络插件安装失败。网络插件是必须的，集群中的应用需要通过网络插件进行沟通，<strong>但是k8s并不自带网络插件</strong>。本次选择了flannel作为我们的网络插件，由于不可联网，首先我们需要通过可以联网的将相关的yml文件下载到本地然后传输到服务器。<a href="https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml" target="_blank" rel="noopener">https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</a><br>然后执行<code>kubectl -f kube-flannel.yml</code> 开始部署网络插件。部署需要几秒钟，可以执行命令<code>kubectl get pods --all-namespaces</code>查看当前的部署状态，如果一段时间过去了所有的POD并不都是running状态，那么说明安装是失败的，那么需要一一检查上面说到步骤，检查kubeadm init的输出中有没有相关有用的信息。极有可能和防火墙以及selinux有关。</p><p><strong>5. 将主节点做为工作节点</strong></p><p>k8s不推荐将master节点做为工作节点，当然如果在资源有限的情况下，可以将主节点变为工作节点。执行下面的语句之后主节点也可以跑容器。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl taint nodes --all node-role.kubernetes.io/master-</span><br></pre></td></tr></table></figure></p><p><strong>6. 加入工作节点</strong></p><p>在工作节点上输入刚才保存的kubeadm join那一串就可以将一个节点加入到集群中，加入的语句是这样的：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeadm join --token &lt;token&gt; &lt;master-ip&gt;:&lt;master-port&gt; --discovery-token-ca-cert-hash sha256:&lt;<span class="built_in">hash</span>&gt;</span><br></pre></td></tr></table></figure></p><p>在<strong>主节点</strong>上执行<code>kubectl get nodes</code>可以看到当前的节点情况，如果很长一段时间都不是ready状态，说明加入有问题。我遇到过这个问题，经过一系列排查，发现是子节点上没有相关的镜像。按照上面提到的步骤，将一些镜像传输到节点上就可以了。如果遇到问题，执行<code>journalctl -r -u kubelet</code> 来找出问题所在吧（主节点和子节点的问题描述会不一致）！最终的输出如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get nodes</span><br><span class="line">NAME                      STATUS    ROLES     AGE       VERSION</span><br><span class="line">node1   Ready     &lt;none&gt;    3d        v1.10.0</span><br><span class="line">node2   Ready     &lt;none&gt;    3d        v1.10.0</span><br><span class="line">master   Ready     master    3d        v1.10.0</span><br></pre></td></tr></table></figure></p><p>安装过程如果有错误，要先执行<code>kubeadm reset</code>，不然无法重新安装。</p><p>至此，一个集群的安装就已经完成了，后续将安装dashboard。dashboard虽然只是一个web ui，但是它有管理的权限，所以安装dashboard还需要额外的安全配置，内容详见下一篇文章。</p>]]></content>
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> docker </tag>
            
            <tag> k8s </tag>
            
            <tag> kubernetes </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Bash学习之变量进阶</title>
      <link href="/2018/03/20/Bash%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%98%E9%87%8F%E8%BF%9B%E9%98%B6/"/>
      <url>/2018/03/20/Bash%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%98%E9%87%8F%E8%BF%9B%E9%98%B6/</url>
      <content type="html"><![CDATA[<blockquote><p>写脚本时候常常需要用到变量，但是Linux中的变量和一般编程语言中的变量差别较大，这里罗列一些比较细节的用法。</p></blockquote><h1 id="变量的特殊用法"><a href="#变量的特殊用法" class="headerlink" title="变量的特殊用法"></a>变量的特殊用法</h1><ol><li><code>$?</code> 表示上一个语句的执行结果</li><li><code>$#</code> 脚本或者函数输入的参数数量</li><li><code>$0</code> 表示脚本名称</li><li><code>$1-n</code> 表示具体输入的第几个变量</li><li><code>$@</code> 表示所有变量</li><li><code>${!name}</code> 间接引用，表示将<code>$name</code>这个变量的值作为一个变量的名称，如下：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">export</span> xyzzy=plugh ; <span class="built_in">export</span> plugh=cave</span><br><span class="line">$ <span class="built_in">echo</span> <span class="variable">$&#123;xyzzy&#125;</span>  <span class="comment"># normal, xyzzy to plugh</span></span><br><span class="line">plugh</span><br><span class="line">$ <span class="built_in">echo</span> <span class="variable">$&#123;!xyzzy&#125;</span> <span class="comment"># indirection, xyzzy to plugh to cave</span></span><br><span class="line">cave</span><br></pre></td></tr></table></figure></li></ol><p>这个特殊用法还有一个用法，<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">6. **shift**命令，shift可以将参数依次轮询，即通过`$1`就可以访问所有元素。注意这个不会影响`$0`。shifit可以接受参数，指定跳跃多少个参数。但是如果跳跃的值大于了当前的`$#`，那么参数不会受影响，而shift则会返回0。下面这个案列展示了这个效果：</span><br><span class="line">```bash</span><br><span class="line">#!/bin/bash</span><br><span class="line"># shift-past.sh</span><br><span class="line">shift 3 # Shift 3 positions.</span><br><span class="line">n=3; shift $n</span><br><span class="line"># Has the same effect.</span><br><span class="line">echo &quot;$1&quot;</span><br><span class="line">exit 0</span><br><span class="line"># ======================== #</span><br><span class="line">until [ -z &quot;$1&quot; ]</span><br><span class="line">do</span><br><span class="line">    echo -n &quot;$1 &quot;</span><br><span class="line">    shift 20 # If less than 20 pos params,</span><br><span class="line">done #+ then loop never ends!</span><br><span class="line"># When in doubt, add a sanity check. . . .</span><br><span class="line">shift 20 || break</span><br></pre></td></tr></table></figure></p><ol start="7"><li><strong>let</strong>指令用于变量数值的加减。</li><li>字符串在加减中默认为0</li></ol>]]></content>
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bash </tag>
            
            <tag> variable </tag>
            
            <tag> shell </tag>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Bash学习之特殊符号</title>
      <link href="/2018/03/19/Bash%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%89%B9%E6%AE%8A%E7%AC%A6%E5%8F%B7/"/>
      <url>/2018/03/19/Bash%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%89%B9%E6%AE%8A%E7%AC%A6%E5%8F%B7/</url>
      <content type="html"><![CDATA[<blockquote><p>bash中很多符合都有不同的用处，有一些符号的用法初看极其晦涩，给阅读和写代码带来了不少障碍，今天就系统学习一下这些特殊符号用法。本次阅读的书籍是来自Mendel Cooper的《Advanced Bash-Scripting Guide》，Bash脚本高级进阶。<br><a id="more"></a></p></blockquote><h1 id="号"><a href="#号" class="headerlink" title="#号"></a>#号</h1><p>#号中文念作jing，在英文中经常念作hash或者pound，在一些其他领域念作Sharp。在脚本编程中经常可以看到#作为注释前置符，这也是我们最熟悉的。<br>下面罗列#的特殊用法：</p><ol><li><strong>作为Bash 脚本的第一行中的#！</strong>（这个组合符号念作shebang，这是特殊读法），这个虽然和注释类似，却有实际的功能，用来表示脚本的语法，常见的就是#!/bin/bash</li><li><strong>在变量，sed等工具中做一些特殊用法</strong>，比如<code>echo ${PATH#*:}</code>，用来截取字符串</li><li>#做为一个特殊的字符，在字符串中大部分不需要转义，除了下面的例子：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> The \<span class="comment"># here does not begin a comment.</span></span><br><span class="line"><span class="built_in">echo</span> The <span class="comment"># here begins a comment.</span></span><br></pre></td></tr></table></figure></li></ol><p>上的例子之所要转义是因为没有被引号包围，被引号包围的#是不会把后面的内容当做注释的。写到这里，我已经转义了不少#号了（markdown也需要转义）。</p><h1 id="；分号"><a href="#；分号" class="headerlink" title="；分号"></a>；分号</h1><p>分号（semicolon）将连个命令表示在同一行，注意分号后面要有空格，最经典的就是if语句中then前面的那个空格。<br><strong>；；</strong>双分号用于case语句中一个选项的结束。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="string">"<span class="variable">$variable</span>"</span> <span class="keyword">in</span></span><br><span class="line">abc) <span class="built_in">echo</span> <span class="string">"\$variable = abc"</span> ;;</span><br><span class="line">xyz) <span class="built_in">echo</span> <span class="string">"\$variable = xyz"</span> ;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure></p><h1 id="点号"><a href="#点号" class="headerlink" title=".点号"></a>.点号</h1><p>.(念作dot)这个符号出现的频率比较高，下面罗列它的一些用法。</p><ol><li>作为一个命令，就是source的缩写，和source用法一致</li><li>作为文件名的一部分，如果在开头，表示隐藏文件，如果在中间，则往往用于区分后缀等。</li><li>在文件夹目录中，<strong>.</strong>表示当前目录，<strong>..</strong>表示上级目录</li><li>在正则表达式中用于表示任意单个字符</li></ol><h1 id="“双引号"><a href="#“双引号" class="headerlink" title="“双引号"></a>“双引号</h1><p>引用一个字符串，但是在这个字符串中很多特殊字符都会有特殊作用，不会进行转义。最常见的就是$表示的变量。</p><h1 id="‘单引号"><a href="#‘单引号" class="headerlink" title="‘单引号"></a>‘单引号</h1><p>和双引号相比最大的区别就是它回转义大部分特殊字符，即大部分都会所见即所得，不会有任何转换。</p><h1 id="逗号"><a href="#逗号" class="headerlink" title=",逗号"></a>,逗号</h1><p>表示一系列操作，但是只有最后一个返回值。其他时候常常用于数组</p><h1 id="反引号"><a href="#反引号" class="headerlink" title="` 反引号"></a>` 反引号</h1><p>反引号常常用于一串命令的输出作为一个变量</p><h1 id="冒号"><a href="#冒号" class="headerlink" title=":冒号"></a>:冒号</h1><p>冒号在Bash中表示什么都不做，但是它的返回值总是0（true），常常用于占位符，也用于下面的用法：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">: &gt; data.xxx <span class="comment"># File "data.xxx" now empty.</span></span><br><span class="line"><span class="comment"># Same effect as cat /dev/null &gt;data.xxx</span></span><br><span class="line"><span class="comment"># However, this does not fork a new process, since ":" is a builtin.</span></span><br></pre></td></tr></table></figure></p><p>和cat /dev/null类似，但是更快</p><h1 id="中括号"><a href="#中括号" class="headerlink" title="[]中括号"></a>[]中括号</h1><p>中括号常常表示test指令，用于if-then语句。当然也能用于数组索引，区别就在于中间有没有空格。</p><h1 id="竖线符号"><a href="#竖线符号" class="headerlink" title="|竖线符号"></a>|竖线符号</h1><p>竖线在Bash中表示<strong>管道</strong>，管道是Bash的经典语法</p><p>还有其他符号诸如%,$,&amp;等就不再赘述了，其用法相对固定。</p>]]></content>
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bash </tag>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>docker常用指令汇总</title>
      <link href="/2018/03/19/docker%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4%E6%B1%87%E6%80%BB/"/>
      <url>/2018/03/19/docker%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4%E6%B1%87%E6%80%BB/</url>
      <content type="html"><![CDATA[<blockquote><p>在这里记录一些docker常用指令，便于后续使用时查询</p></blockquote><h1 id="docker相关"><a href="#docker相关" class="headerlink" title="docker相关"></a>docker相关</h1><h2 id="1-系统使用"><a href="#1-系统使用" class="headerlink" title="1. 系统使用"></a>1. 系统使用</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#开机启动docker</span></span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line"><span class="comment">#启动docker</span></span><br><span class="line">systemctl start docker</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="2-docker容器相关操作"><a href="#2-docker容器相关操作" class="headerlink" title="2. docker容器相关操作"></a>2. docker容器相关操作</h2><blockquote><p>docker容器可以理解为在沙盒中运行的进程。这个沙盒包含了该进程运行所必须的资源，包括文件系统、系统类库、shell 环境等等。但这个沙盒默认是不会运行任何程序的。你需要在沙盒中运行一个进程来启动某一个容器。这个进程是该容器的唯一进程，所以当该进程结束的时候，容器也会完全的停止。</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动、停止等操作</span></span><br><span class="line">docker start|stop|restart [id]</span><br><span class="line"><span class="comment"># 暂停|恢复 某一容器的所有进程</span></span><br><span class="line">docker pause|unpause [id]</span><br><span class="line"><span class="comment"># 杀死一个或多个指定容器进程</span></span><br><span class="line">docker <span class="built_in">kill</span> -s KILL [id]</span><br><span class="line"><span class="comment"># 停止全部运行的容器</span></span><br><span class="line">docker stop `docker ps -q`</span><br><span class="line"><span class="comment"># 杀掉全部运行的容器</span></span><br><span class="line">docker <span class="built_in">kill</span> -s KILL `docker ps -q`</span><br><span class="line"><span class="comment"># 查看当前运行的容器</span></span><br><span class="line">docker ps</span><br><span class="line"><span class="comment"># 查看全部容器</span></span><br><span class="line">docker ps -a</span><br><span class="line"><span class="comment"># 查看全部容器的id和信息</span></span><br><span class="line">docker ps -a -q</span><br><span class="line"><span class="comment"># 查看一个正在运行容器进程，支持 ps 命令参数</span></span><br><span class="line">docker top</span><br><span class="line"><span class="comment"># 查看容器的示例id</span></span><br><span class="line">sudo docker inspect -f  <span class="string">'&#123;&#123;.Id&#125;&#125;'</span> [id]</span><br><span class="line"><span class="comment"># 检查镜像或者容器的参数，默认返回 JSON 格式</span></span><br><span class="line">docker inspect</span><br><span class="line"><span class="comment"># 返回 ubuntu:14.04  镜像的 docker 版本</span></span><br><span class="line">docker inspect --format <span class="string">'&#123;&#123;.DockerVersion&#125;&#125;'</span> ubuntu:14.04</span><br><span class="line"><span class="comment"># 创建一个容器命名为 test 使用镜像ubuntu</span></span><br><span class="line">docker create -it --name <span class="built_in">test</span> ubuntu</span><br><span class="line"><span class="comment"># 创建并启动一个容器 名为 test 使用镜像ubuntu</span></span><br><span class="line">docker run --name <span class="built_in">test</span> ubuntu</span><br><span class="line"><span class="comment"># 删除一个容器，删除之前需要先停止，或者也可以加-f参数强制停止</span></span><br><span class="line">docker rm [容器id]</span><br><span class="line"><span class="comment"># 删除所有容器</span></span><br><span class="line">docker rm `docker ps -a -q`</span><br></pre></td></tr></table></figure><h2 id="3-docker-镜像相关操作"><a href="#3-docker-镜像相关操作" class="headerlink" title="3. docker 镜像相关操作"></a>3. docker 镜像相关操作</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 根据Dockerfile 构建</span></span><br><span class="line">docker build -t [image_name] [Dockerfile_path]</span><br><span class="line"><span class="comment"># 列出本地所有镜像</span></span><br><span class="line">docker images ls </span><br><span class="line"><span class="comment"># 本地镜像名为 ubuntu 的所有镜像</span></span><br><span class="line">docker images ubuntu</span><br><span class="line"><span class="comment"># 查看指定镜像的创建历史</span></span><br><span class="line">docker <span class="built_in">history</span> [id]</span><br><span class="line"><span class="comment"># 本地移除一个或多个指定的镜像</span></span><br><span class="line">docker rmi</span><br><span class="line"><span class="comment"># 移除本地全部镜像</span></span><br><span class="line">docker rmi `docker images -a -q`</span><br><span class="line"><span class="comment"># 指定镜像保存成 tar 归档文件， docker load 的逆操作</span></span><br><span class="line">docker save</span><br><span class="line"><span class="comment"># 删除所有标记为None的镜像</span></span><br><span class="line">docker rmi $(docker images -f “dangling=<span class="literal">true</span>” -q)</span><br><span class="line"><span class="comment"># 将镜像 ubuntu:14.04 保存为 ubuntu14.04.tar 文件</span></span><br><span class="line">docker save -o ubuntu14.04.tar ubuntu:14.04</span><br><span class="line"><span class="comment"># 从 tar 镜像归档中载入镜像， docker save 的逆操作</span></span><br><span class="line">docker load</span><br><span class="line"><span class="comment"># 上面命令的意思是将 ubuntu14.04.tar 文件载入镜像中</span></span><br><span class="line">docker load -i ubuntu14.04.tar</span><br><span class="line">docker load &lt; /home/save.tar</span><br><span class="line"><span class="comment"># 构建自己的镜像</span></span><br><span class="line">docker build -t &lt;镜像名&gt; &lt;Dockerfile路径&gt;</span><br><span class="line">docker build -t xx/tomcat .</span><br></pre></td></tr></table></figure><h1 id="Docker-Compose相关"><a href="#Docker-Compose相关" class="headerlink" title="Docker-Compose相关"></a>Docker-Compose相关</h1>]]></content>
      
      
    </entry>
    
    <entry>
      <title>ITDocker资料清单</title>
      <link href="/2018/03/17/ITDocker%E8%B5%84%E6%96%99%E6%B8%85%E5%8D%95/"/>
      <url>/2018/03/17/ITDocker%E8%B5%84%E6%96%99%E6%B8%85%E5%8D%95/</url>
      <content type="html"><![CDATA[<h1 id="ITDocker端口使用清单"><a href="#ITDocker端口使用清单" class="headerlink" title="ITDocker端口使用清单"></a>ITDocker端口使用清单</h1><table><thead><tr><th>端口</th><th>服务</th></tr></thead><tbody><tr><td>5000</td><td>IT课Docker私有Registry</td></tr><tr><td>5001</td><td>IT课Docker私有Registry可视化工具</td></tr><tr><td>9000</td><td>portainer，Docker管理程序</td></tr><tr><td>9001</td><td>Jenkins，CI服务</td></tr><tr><td>9002</td><td>ITDocker文档整理</td></tr><tr><td>26379-26381</td><td>redis_sentinel，一共有3个哨兵</td></tr><tr><td>50000</td><td>Jenkins 安全管理Agent端口</td></tr></tbody></table><p>请使用<strong>itdocker.rd.tp-link.net:PORT</strong>访问相关应用，开发人员用于开发和测试端口一般在<strong>8000-8999</strong>之间，这些端口全公司都可以访问。<br><a id="more"></a></p><h1 id="ITDocker自制镜像一览"><a href="#ITDocker自制镜像一览" class="headerlink" title="ITDocker自制镜像一览"></a>ITDocker自制镜像一览</h1><blockquote><p>具体这些镜像的信息请在<a href="itdocker.rd.tp-link.net:5001" target="_blank" rel="noopener">私有镜像管理器</a>查看</p></blockquote><table><thead><tr><th>镜像名称</th><th>特性</th></tr></thead><tbody><tr><td>itdocker.rd.tp-link.net:5000/itmaven:sencha</td><td>集成了sencha，并且配置了公司内网MAVEN仓库</td></tr><tr><td>itdocker.rd.tp-link.net:5000/itjenkins:latest</td><td>集成了Docker-Compose</td></tr><tr><td>itdocker.rd.tp-link.net:5000/ittomcat:latest</td><td>修改了Server.XML，以便自动部署</td></tr><tr><td>itdocker.rd.tp-link.net:5000/itmaven:latest</td><td>不包括sencha的轻量级maven，配置了公司内网MAVEN仓库</td></tr><tr><td>itdocker.rd.tp-link.net:5000/redis_sentinel:alpine</td><td>自制redis_sentinel，主要用于Docker-Compose，不适合直接生成容器</td></tr></tbody></table>]]></content>
      
      <categories>
          
          <category> 系统管理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> system </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>在docker中配置jenkins自动化部署</title>
      <link href="/2018/03/15/%E5%9C%A8docker%E4%B8%AD%E9%85%8D%E7%BD%AEjenkins/"/>
      <url>/2018/03/15/%E5%9C%A8docker%E4%B8%AD%E9%85%8D%E7%BD%AEjenkins/</url>
      <content type="html"><![CDATA[<h1 id="在docker中配置Jenkins"><a href="#在docker中配置Jenkins" class="headerlink" title="在docker中配置Jenkins"></a>在docker中配置Jenkins</h1><h2 id="Jenkins介绍"><a href="#Jenkins介绍" class="headerlink" title="Jenkins介绍"></a>Jenkins介绍</h2><blockquote><p>Jenkins主要用于持续集成（Continuous Integration，CI），CI是现代软件开发领域的基石，它改变了团队对于整个开发过程的理解。一个好的CI架构能够使得从开发到部署顺序进行，更快地发现和修复bug,最终给客户带来更多的价值。每个专业的开发团队，无论大还是小都应该采用CI。CI的软件很多，我司采用了jenkins这个软件，下面主要介绍这个软件的在Docker中的搭建和配置。</p></blockquote><p>Jenkins起源于Hudson,占据了很大的市场份额，可被各种大小的团队和用不同语言(.NET,Ruby,Groovy,Grails,PHP等)开发的项目使用。优点如下：</p><ul><li>易用性；</li><li>可扩展。插件覆盖了版本控制系统，构建工具，代码质量度量工具，通知工具，和其它外部系统进行集成，UI自定义等；</li><li>活跃的社区；</li><li>稳定版本的支持。Long-term Support(LTS)发布。</li></ul><p>除了上述优点，有了docker的加持，Jenkins更是像神灵附体，CI能力更上一层楼。我在本次调研中主要感受到了以下几个优点：<br><a id="more"></a></p><ul><li>极速安装，极速部署，不需要知道jenkins依赖什么，如何安装，甚至不需要知道jenkins是干嘛的，在docker中pull一下就能打开jenkins主页，非常舒畅</li><li>部署测试需要用的工具例如maven,nodejs不需要事先为其搭建，需要用到的时候只要从docker中pull一个即可。</li><li>代码直接部署到一个全新的环境中，基本上很多软件不需要配置文件（端口之类的都不用改），非常舒畅</li></ul><p><strong>由于本次调研只是初步尝鲜，对于利用docker实现CI的局限性了解还不够深入，需要后续观察。</strong></p><hr><h2 id="Jenkins的安装与环境配置"><a href="#Jenkins的安装与环境配置" class="headerlink" title="Jenkins的安装与环境配置"></a>Jenkins的安装与环境配置</h2><h3 id="1-Jenkins的安装"><a href="#1-Jenkins的安装" class="headerlink" title="1. Jenkins的安装"></a>1. Jenkins的安装</h3><p>由于网络原因，本次Jenkins安装是在离线状态下进行的，所以docker也要从私有仓库上拉，安装过程没有可以联网的情况下那么方便，所以就采用了在有网的环境中部署，然后复制到内网环境中。我的PC是上有网，所以在PC上装了一个docker(强烈推荐像我一样的新手这么做，不仅有网方便，其次在PC上开发比linux上开发舒服多了)。在PC上安装docker之前，也强烈推荐将操作系统升级到windows 10，主要是因为windows10的虚拟化功能。在其他版本的操作系统上需要安装一个虚拟机，然后在虚拟机里运行docker，相比windows10的原生支持虚拟要差很多。<strong>当然能有一个直接联网的linux服务器就更好了。</strong></p><p>在windows 10和linux中运行Jenkins的命令差不多，需要注意的是，jenkins官方镜像是<strong>jenkins/jenkins</strong>而不是<strong>jenkins</strong>(一般官方镜像是没有前缀的，但是jenkins不是)，这个镜像不包含一些额外的插件。jenkins教程中推荐的镜像是<strong>jenkinsci/blueocean</strong>，它集成了一个叫blue occean的插件，这个插件在UI上做了很多的改进，相比原来的UI，好看不少。除此之外，blueoccean的配置也比较简单（如果能够联网的话），所以本次实验使用的是blueocean版本的jenkins。输入下面的命令运行jenkins：<br><figure class="highlight bat"><table><tr><td class="code"><pre><span class="line">docker run ^</span><br><span class="line">  --rm ^</span><br><span class="line">  -u root ^</span><br><span class="line">  -p <span class="number">8080</span>:<span class="number">8080</span> ^</span><br><span class="line">  -v G:\DockerData\jenkins:/var/jenkins_home ^</span><br><span class="line">  -v /var/run/docker.sock:/var/run/docker.sock ^</span><br><span class="line">  -v "<span class="variable">%HOMEPATH%</span>":/home ^</span><br><span class="line">  --network IT_DOCKER_NET ^</span><br><span class="line">  --name Jenkins ^</span><br><span class="line">  itdocker.<span class="built_in">rd</span>.tp-link.<span class="built_in">net</span>:<span class="number">5000</span>/jenkinsci/blueocean</span><br></pre></td></tr></table></figure></p><p>在linux中的命令类似，改变一下数据卷路径即可。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run \</span><br><span class="line">  -u root \</span><br><span class="line">  --rm \  </span><br><span class="line">  -d \ </span><br><span class="line">  -p 8080:8080 \ </span><br><span class="line">  -p 50000:50000 \ </span><br><span class="line">  -v /opt/docker_data/jenkins_data:/var/jenkins_home \ </span><br><span class="line">  -v /var/run/docker.sock:/var/run/docker.sock \</span><br><span class="line">  --network IT_DOCKER_NET \</span><br><span class="line">  --name Jenkins \</span><br><span class="line">  rdsource.tp-link.net:8088/jenkinsci/blueocean</span><br></pre></td></tr></table></figure></p><p>执行上述命令之后，就可以通过本地的8080端口访问了。上述命令需要说明就是<code>-v /var/run/docker.sock:/var/run/docker.sock</code>这个命令，这个命令可以实现在jenkins这个容器中创建其他容器，即<em>docker-in-docker</em>，具体的原理可以参考<a href="https://medium.com/lucjuggery/about-var-run-docker-sock-3bfd276e12fd" target="_blank" rel="noopener">About /var/run/docker.sock</a>，了解这个原理之后就可以感叹linux everything is file的理念。</p><hr><h3 id="2-jenkins的配置"><a href="#2-jenkins的配置" class="headerlink" title="2.jenkins的配置"></a>2.jenkins的配置</h3><ul><li>插件配置</li></ul><p>等待docker拉取镜像完毕，在浏览器中输入相应的地址和端口，就可以进入jenkins初始化界面。最初进入了界面时需要一个密钥，密钥在启动日志中，可以输入<code>docker container logs</code>查询刚才容器的日志，里面提到了这个密码。输入这个密码之后就是安装插件，选择安装推荐的插件，推荐的插件不一定都能安装上，如下图：</p><p>如果遇到这种情况，首先先选择跳过，然后进入jenkins页面中，点击如下路径：<strong>系统管理</strong>-&gt;<strong>管理插</strong>件-&gt;<strong>高级</strong>，在下图位置修改URL，<br><img src="/2018/03/15/在docker中配置jenkins/2018-03-16_095736.jpg" title="更改update_center URL"><br>造成插件无法安装的主要原因还是因为一些不可描述的原因。可以在下面的链接中选择合适的镜像源，<a href="http://mirrors.jenkins-ci.org/status.html" target="_blank" rel="noopener">jenkins镜像源</a>,之后需要手动安装那些没有安装上的插件。安装就在插件管理中安装即可，这里就不多赘述了。除了上述推荐的插件，还有一个<strong>Gerrit Trigger</strong>的插件需要安装，这个插件主要用于和公司gerrit集成。记得安装完所有插件之后重启jenkins。</p><ul><li>git配置</li></ul><p>jenkins需要和gerrit集成，所以首先需要配置git，git的配置和以前是相同的，进入jenkins的bash中（如何进入可以参考以前的文档），按照下面的步骤配置即可：</p><ol><li>生成公私密钥ssh-keygen -t rsa -C <a href="mailto:songxuetao@tp-link.com.cn" target="_blank" rel="noopener">songxuetao@tp-link.com.cn</a><br>之后多次输入回车默认即可。当然可以输入密码保护自己的密钥。</li><li>复制公钥到gerrit<br>将~.ssh/id_rsa.pub中公钥复制到gerrit中</li><li>添加config文件<br>vi ~/.ssh/config,输入下面的信息保存即可（<em>下面的信息是告诉git如何使用公钥密钥，由于我司的gerrit较古老，还在用较不安全的diffie-hellman-group1-sha1加密算法，所以必须在git中配置下面的信息，不然git默认不会使用这种算法</em>）<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Host *</span><br><span class="line">KexAlgorithms +diffie-hellman-group1-sha1</span><br></pre></td></tr></table></figure></li></ol><p>配置完上述git之后，就可以让jenkins直接拉取gerrit的代码了。</p><ul><li>gerrit trigger配置</li></ul><p>点击<strong>系统配置</strong>，在最后找到<strong>Gerrit Trigger</strong>配置项，在其中配置gerrit服务器。进入页面之后在左上角选择<strong>Add New Server</strong>，输入名称之后，勾选默认，然后进入配置界面。配置界面主要配置gerrit服务器的IP地址端口以及SSH传输数据用到的密钥和公钥等，最终配置如下图：<br><img src="/2018/03/15/在docker中配置jenkins/2018-03-16_114753.jpg" title="gerrit"><br>如果gerrit配置的SSH端口不是8000，那么需要修改上述的端口，配置完成之后点击Test Connection测试一下，看到success之后就大功告成了。</p><ul><li>其他配置</li></ul><p>如果想要让Jenkins能够发送邮件，还需要做一些配置。点击<strong>系统配置</strong>，找到下面的选项——<strong>Extended E-mail Notification</strong>：<br>点击高级，输入相关的邮箱设置，包括服务器端口，发件人密码等等（大部分默认即可），一些高级特性目前还用不上，最终效果如下：<br><img src="/2018/03/15/在docker中配置jenkins/2018-03-16_113413.jpg" title="配置邮箱"></p><p>如果要启用LDAP验证（我使用了一下LDAP验证，效果不好，无法实现访问控制，要实现需要从LDAP着手解决，比较麻烦。所以最终没有启用，选择了手动管理用户），也同样在这个页面中找到下面的配置项，输入相关信息即可。配置完成之后还需要在安全设置中启用LDAP，启用之后我司员工就能登录了。<br><img src="/2018/03/15/在docker中配置jenkins/2018-03-16_114124.jpg" title="配置LDAP"></p><hr><h2 id="3-jenkins使用入门介绍"><a href="#3-jenkins使用入门介绍" class="headerlink" title="3.jenkins使用入门介绍"></a>3.jenkins使用入门介绍</h2><blockquote><p>以前我司的jenkins的自动部署用到了很多SHELL脚本，并且脚本的执行很依赖于环境（即换个地方就很难运行起来了），此次引入了docker进行自动化部署，相比以往的自动化部署稍微要复杂一些（原因是我们的Jenkins部署在Docker中，无法直接运行宿主机中的程序），需要写一些脚本，增加了学习成本。此次之所以用脚本的原因在于脚本的可扩展性高，且易于维护，一旦jenkins崩溃了不会影响自动化部署流程。Jenkins用的脚本是一个名为jenkinsfile的文件（没有任何后缀，和dockerfile一样），在这个脚本写一些代码，Jenkins就会按照脚本中写的步骤依次执行。下面演示以SMBCommunity这个项目为例，做一个说明。</p></blockquote><p>SMBCommunity的架构是Java+ExtJS+Spring，项目用maven构建，编译整个项目需要用到maven和sencha，Web服务器用的则是开源的Tomcat。如果不做测试的话，整个自动化的流程如下图：<br><img src="/2018/03/15/在docker中配置jenkins/2018-03-16_155220.jpg" title="项目自动化流程"><br>同时构建触发可以是手动触发，也可以是代码触发，即这个项目里有了新的代码，那么Jenkins就会为它重新构建一个Tomcat应用。</p><h3 id="blue-ocean介绍"><a href="#blue-ocean介绍" class="headerlink" title="blue ocean介绍"></a>blue ocean介绍</h3><p>相比以前，<strong>blue ocean</strong> 主要是在UI上做了很大的改进，在视觉体验上更现代化，也更简洁大方，下图是blue ocean首页效果。<br><img src="/2018/03/15/在docker中配置jenkins/2018-03-17_100752.jpg" title="blue ocean首页"><br>除了在UI上有了很大的改进，blue occean也支持在线编辑pipeline，同时原生支持Github的pull request。相比于Jenkins经典UI，blue ocean对构建过程的可视化效果也要好很多，不同的背景代表了构建的结果，比如红色代表失败，绿色代表成功。</p><h3 id="操作流程"><a href="#操作流程" class="headerlink" title="操作流程"></a>操作流程</h3><p>下面就按照这个流程在Jenkins中设置。为了演示方便，我需要添加一些新的文件，我Fork了一份代码，在Gerrit中获取其SSH路径。然后在Jenkins中选择新建任务，在任务类型中选择<strong>流水线</strong>（pipeline）,如下图<br><img src="/2018/03/15/在docker中配置jenkins/2018-03-16_160134.jpg" title="新建一个流水线"><br>在打开的配置中，有几个配置要注意，第一个是构建触发器，选择Gerrit event，也可以选择周期性构建，但是周期性构建有一点浪费资源。选择Gerrit event之后会展开一些选项，在其中选择我们上面设置的Gerrit服务器，Trigger on 选择<strong>Change Merged</strong>，Gerrit项目中填写相关的项目名称和分支，这部分最终填完是下图的样子。<br><img src="/2018/03/15/在docker中配置jenkins/2018-03-16_161631.jpg" title="Gerrit Trigger设置"><br>在流水线中选择Pipeline script from SCM，这个选项意思是jenkinsfile从源码中读取，所以在我们Fork的源码根目录下我们需要添加一个文件叫jenkinsfile。选择这个项目之后就会展开让你选择Git的URL（如果不选择这个选项的话，需要自己写脚本从某个URL拉取代码，比较麻烦），SCM选择Git，将从Gerrit上拷贝的URL复制到Repository URL，输入想要Build的分支，最终填写如下：<br><img src="/2018/03/15/在docker中配置jenkins/2018-03-16_162348.jpg" title="流水线设置"><br>上面几个都填写完毕之后保存即可。这样一来，每当Gerrit有代码通过审核之后，Jenkins就会启动一次构建。如何构建是写在jenkinsfile中的。</p><h3 id="撰写jenkinsfile"><a href="#撰写jenkinsfile" class="headerlink" title="撰写jenkinsfile"></a>撰写jenkinsfile</h3><p>在源代码中创建一个Jenkinsfile，有下面这些好处：</p><ol><li>Pipeline上的代码审查/迭代</li><li>Pipeline的审计跟踪</li><li>Pipeline的唯一真实来源，可以由项目的多个成员查看和编辑。</li></ol><p>Jenkinsfile实际上对应的就是一个Pipeline脚本,Pipeline支持两种语法：<strong>Declarative</strong>（在Pipeline 2.5中引入）和<strong>Scripted</strong> Pipeline。两者都支持建立一个连续流程Pipeline。两者都可以用于在Web UI或者源代码中中定义一个流水线Jenkinsfile，但Jenkins推荐的做法也是将Jenkinsfile放入源码中，而不是直接在Jenkins中设置。上面提到的两种语法中，学习难度较小的是Declarative，本文也是用的Declarative，Scripted Pipeline是jenkins的基础，但是容易度和可阅读性上都比Declarative差一点，所以我没有学Scripted的相关语法，从学习结果看，Declarative语法已经足够我们开发人员使用了。下面的代码如果不加说明都是Declarative语法。</p><h4 id="创建Jenkins文件"><a href="#创建Jenkins文件" class="headerlink" title="创建Jenkins文件"></a>创建Jenkins文件</h4><p> Jenkinsfile是一个包含Jenkins Pipeline定义的文本文件，并被检入源代码控制，pipeline中每一个步骤的属性时通过括号和缩进控制的。下面贴上的代码是一个包含三个顺序执行过程的Pipeline。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line"></span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&apos;Build&apos;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &apos;Building..&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&apos;Test&apos;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &apos;Testing..&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&apos;Deploy&apos;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &apos;Deploying....&apos;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>pipeline是最外层元素，表示这是一个Declarative 语法pipeline，它的括号里面是关于这个pipeline具体执行过程的信息。<code>agent</code>表示这些步骤在哪些环境中执行，通常有两种any和docker，any一般都是直接在jenkins中执行，而docker则是生成一个容器执行命令，本次实验中一部分命令需要在docker中执行。<code>stages</code>表示一系列执行环节，从上面的代码中可以看到它是由一些stage组成的（上面代码中包括了Build，TEST，Deploy三个环节）。另外<code>agent</code>元素和<code>stages</code>元素表明这些环节默认的执行环境都是any，当然也可以在<code>stages</code>的子元素<code>stage</code>中再定义的一个agent，这个agent会覆盖默认的设置生效。在一个步骤<code>stage</code>中，会包含一些步骤<code>steps</code>,这些步骤会按照书写的顺序依次执行。上面的步骤只是简单的输出几个语句。<br>将上面的代码复制到项目源代码根目录下的jenkinsfile中，然后推送到gerrit上评审。commit消息写 Add Jenkinsfile。打开Gerrit中，将上述commit的评审通过。一旦评审通过，立刻打开jenkins，在右边选择open blueocean，就可以看到如下消息：<br><img src="/2018/03/15/在docker中配置jenkins/2018-03-16_173202.jpg" title="自动触发构建"><br>从图上看，一旦评审通过，那么gerrit就会自动构建一次。点开这次构建查看具体信息。如下图：<br><img src="/2018/03/15/在docker中配置jenkins/2018-03-16_173417.jpg" title="构建成功截图"><br>绿色的背景代表了构建是成功的，具体构建的信息可以在Log中找到。至此，在jenkins中自动化部署的简单示例就完成了，接下来就需要完善Jenkinsfile，使其能够真真满足实际工作需要。关于Jenkinsfile脚本的语法可以在<a href="https://jenkins.io/doc/book/pipeline/" target="_blank" rel="noopener">官网链接</a>上找到，稍微熟悉一下脚本之后，可以使用jenkins主页上的片段生成器生成相应的代码，参考链接：<a href="http://itdocker.rd.tp-link.net:9001/job/Jenkins_Test/pipeline-syntax/" target="_blank" rel="noopener">片段生成器</a>。</p>]]></content>
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> CI </tag>
            
            <tag> 自动化部署 </tag>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Docker部署指南</title>
      <link href="/2018/03/14/Docker%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97/"/>
      <url>/2018/03/14/Docker%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97/</url>
      <content type="html"><![CDATA[<h1 id="IT-Docker部署简单指南"><a href="#IT-Docker部署简单指南" class="headerlink" title="IT Docker部署简单指南"></a>IT Docker部署简单指南</h1><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><ol><li>我司普遍采用的是centos6，但是Centos7加强了很多虚拟化功能，所以建议在Centos7上部署。CentOS7和6的操作微微有一点差异，会有记号标注。</li><li>关于联网，如果服务器能联网，那么下面的很多步骤可以省略，此次以我司无法连接外网的情况下进行配置。一样很简单。<a id="more"></a></li></ol><hr><h2 id="二、安装Docker"><a href="#二、安装Docker" class="headerlink" title="二、安装Docker"></a>二、安装Docker</h2><p>Docker的安装可以有很多方式，最方便的还是直接利用yum安装。此次让网络管理课添加了Docker-CE的镜像了，可以直接安装，步骤如下：</p><ol><li>配置公司yum仓库和Docker仓库，一共有两个文件。前面一个内网上有，后面一个配置如下：</li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://rdsource.tp-link.net/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line"><span class="comment">###检查下载的文件中的网址是否正确，如果有里面的https://download.docker.com 则替换为http://rdsource.tp-link.net/docker-ce</span></span><br><span class="line">mv docker-ce.repo /etc/yum.repos.d/</span><br><span class="line">yum makecache</span><br><span class="line"><span class="comment">#用下面的指令查看是否有</span></span><br><span class="line">yum list|grep docker-ce</span><br></pre></td></tr></table></figure><p>由于centos7自带Docker，我们要首先卸载掉：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum remove docker \</span><br><span class="line">docker-common \</span><br><span class="line">docker-selinux \</span><br><span class="line">docker-engine</span><br></pre></td></tr></table></figure><p>安装一些依赖包（可选，大部分时候都是自带）</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install -y yum-utils \</span><br><span class="line">device-mapper-persistent-data \</span><br><span class="line">lvm2</span><br></pre></td></tr></table></figure><p>安装docker</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install docker-ce</span><br></pre></td></tr></table></figure><p>配置Docker开机启动（<em>注：这里和6的操作不一样</em>）</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> docker</span><br></pre></td></tr></table></figure><p>配置Docker仓库。我们的服务器连不上网，所以需要连接上公司私有的仓库。<code>vi /etc/docker/daemon.json</code>这个路径如果不存在的话新建就好，另外启动docker也会创建这个路径。由于配置仓库还需要重启，这里就不启动docker而手动创建了。在打开的文件中输入下面的内容。</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"insecure-registries"</span>: [</span><br><span class="line">    <span class="string">"rdsource.tp-link.net:8088"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">"disable-legacy-registry"</span>: <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动docker</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl start docker</span><br><span class="line"><span class="comment">#配置用户名和密码</span></span><br><span class="line">docker login -u admin -p admin123 rdsource.tp-link.net:8088</span><br></pre></td></tr></table></figure><p>测试一下，拉取Hello World，然后运行</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker pull hello-world</span><br><span class="line">docker run hello-world</span><br></pre></td></tr></table></figure><p>如果出现下面的信息，代表Docker已经安装成功。</p><p><img src="http://itdbaas1.rd.tp-link.net:2000/api/getFile?filename=file_1519443696129.jpg" alt="image"></p><p><strong>注意</strong>:上面介绍的语法是默认，在我司无法连接外网的情况下，需要指定registry，我司的地址是rdsource.tp-link.net:8088，所以拉取hello-world的完整语句如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker pull rdsource.tp-link.net:8088/hello-world</span><br><span class="line">docker run rdsource.tp-link.net:8088/hello-world</span><br></pre></td></tr></table></figure><hr><h2 id="三、配置管理镜像"><a href="#三、配置管理镜像" class="headerlink" title="三、配置管理镜像"></a>三、配置管理镜像</h2><p>上述配置都成功之后就需要安装docker的管理工具，由于本次的Docker主要用于测试环境，所以选择了轻量级的Portainer工具来管理Docker。<strong><em>在这里要提一个非常有趣的东西</em></strong>，因为Portainer官方提供的工具也是一个容器（当然也可以非容器），而由于容器访问不了外部环境，那么它是如何管理Docker的呢？docker社区提出了一个非常聪明的解决方案，将docker的socket挂载在容器内部一模一样的虚拟环境中。会有疑问，socket不是通信吗？怎么挂载？这就归功于linux的设计者提出的思想，<strong>Everything is file</strong>，上述的通信实际上可以通过/var/run/docker.sock访问，所以只需要将这个文件挂载在虚拟系统中，那么容器中也能管理本地docker啦。</p><h3 id="1-创建网络"><a href="#1-创建网络" class="headerlink" title="1. 创建网络"></a>1. 创建网络</h3><p>在此之前还需要配置相关的网络，因为我们的portainer需要去访问另外一个容器的端口，所以需要将他们放入同一个网络。用下面的语句创建一个网络：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker network create -d bridge IT_DOCKER_NET</span><br></pre></td></tr></table></figure><p>创建完成之后，以后的容器在启动时只需要加入–network参数即可。</p><p>在配置管理工具之前先创建相关目录：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir -p /opt/docker_data/portainer</span><br></pre></td></tr></table></figure><h3 id="2-部署portainer容器"><a href="#2-部署portainer容器" class="headerlink" title="2. 部署portainer容器"></a>2. 部署portainer容器</h3><p>然后运行下面的命令下载并运行Portainer：</p><p><code>docker run -d -p 9000:9000 --restart always -v /var/run/docker.sock:/var/run/docker.sock -v /opt/docker_data/portainer:/data --network IT_DOCKER_NET rdsource.tp-link.net:8088/portainer/portainer -t &quot;http://localhost:9001/templates.json&quot;</code></p><p>上面的命令中需要解释的就是一个-t，这是portainer提供的模板功能，由于我们无法上网，所以需要搭建本地的模板网站（用的是nginx代理的），在启动的时候需要指向这个本地的网站。后续我们会讲如何搭建本地模板网站，在docker的帮助下非常简单。</p><p>在浏览器中输入地址+端口号9000进入管理页面，如下图：</p><p><img src="http://itdbaas1.rd.tp-link.net:2000/api/getFile?filename=file_1520240556949.jpg" alt="image"></p><p>初始登录时设置管理员用户名和密码。下一步点击管理本地Docker，如下图：</p><p><img src="http://itdbaas1.rd.tp-link.net:2000/api/getFile?filename=file_1520240630812.jpg" alt="image"></p><p>然后就能进入管理界面。</p><h3 id="3-重要更新，利用docker-compose"><a href="#3-重要更新，利用docker-compose" class="headerlink" title="3.重要更新，利用docker-compose"></a>3.重要更新，利用docker-compose</h3><p>上述部署中，portainer和templates是分开部署的，这在实际生产过程中用的比较少，通常更可靠的做法是用一些编排工具编制这些有联系的服务。官方推荐的工具时docker-compose，docker-compose能够将一些容器做成服务，然后组成一个完整的项目，具体的使用可以参考官方文档，这里简单介绍一下在ITDOCKER上的部署。</p><p>首先需要去<a href="https://github.com/docker/compose/releases" target="_blank" rel="noopener">github</a>上下载对应的二进制文件，已经编译好，可以直接放入相关bin目录中使用。下载完成之后记得需要<code>chmod +x</code>。运行<code>docker-compose --version</code>查看是否正常。</p><p>docker-compose也和dockerfile一样基于一个叫docker-compose.yml的文件，在工作目录下vim docker-compose.yml，输入以下内容：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">version: &quot;3&quot;</span><br><span class="line">services:</span><br><span class="line">  portainer-web:</span><br><span class="line">    image: &quot;rdsource.tp-link.net:8088/portainer/portainer&quot;</span><br><span class="line">    command: --templates http://portainer-templates/templates.json</span><br><span class="line">    restart: always</span><br><span class="line">    ports: </span><br><span class="line">      - &quot;9000:9000&quot;</span><br><span class="line">    volumes:</span><br><span class="line">      - &quot;/opt/docker_data/portainer:/data&quot;</span><br><span class="line">      - &quot;/var/run/docker.sock:/var/run/docker.sock&quot;</span><br><span class="line">  portainer-templates:</span><br><span class="line">    image: &quot;portainer-templates&quot;</span><br><span class="line">    volumes:</span><br><span class="line">      - &quot;/opt/docker_data/portainer_template/templates.json:/usr/share/nginx/html/templates.json&quot;</span><br><span class="line">    expose: </span><br><span class="line">      - &quot;80&quot;</span><br></pre></td></tr></table></figure><p>注意需要缩进，从文件中可以看出，portainer-web是我们的可视化管理工具，而portainer-templates是我们的模板仓库。</p><p>模板仓库暴露了80端口，但是只能由compose内部进行访问，外部是无法通过宿主机访问的。外部唯一能访问的就是9000端口。更方便的是前端管理界面可以直接通过服务名访问templates，compose帮我们做了相关的DNS设置。</p><p>输入<code>docker-compose up -d</code> 就可以启动整个应用了。进入管理界面，点击app templates如果看到下面的界面就表示部署成功了。</p><p><img src="http://itdbaas1.rd.tp-link.net:2000/api/getFile?filename=file_1520308152716.jpg" alt="image"></p><p>之后就可以进入相关页面进行设置和管理。</p>]]></content>
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> 部署 </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
